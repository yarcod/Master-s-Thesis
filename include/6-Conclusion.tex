\chapter{Conclusion}
In this work we have evaluated how applications behave and perform when run on a network of memories. Exploiting the integrated capabilities of Hybrid Memory Cube, we notice an overall added latency thus slowing down the execution time; especially using our methodology since we measure a worst-case network scenario using a chain topology with up to seven devices. Our approach has also included non-optmised software and naive scheduling, thereby highlighting issues arising from longer memory traversal time as much as possible. Furthermore, our simulated results suggest an uncertainty in increase of latency due to queues quickly filling up. The abstract, packet-based protocol used enables the CPU simulator to send memory requests through any link and still expect a valid response, since the memory cubes themselves contain internal routing capabilities to the proper vault, but this adds overhead if packets are sent through the wrong link. As such, although using more links, and thereby bandwidth, between devices, this could oftentimes not be utilised because of the internal routing. This could likely be mitigated to some extent by scheduler awareness.
\bigskip

The increase in latency due to network handling and overhead leads to an, at most, around doubled latency compared to ordinary DRAM, while increasing available bandwidth. In addition, there is a large variance in waiting time for requests and it is challenging to predict application behaviour on beforehand. Moreover, trying to exploit a higher degree of BLP without intelligent scheduling can further hurt the average latency, and if the application is unable to take advantage of the added bandwidth it will see a performance degradation as well. Whether a single application is able to benefit from using HMC comes down to its memory access pattern, where a streamed memory access seem to hurt performance the most. However, there does not seem to be any additional adverse effects to applications' execution time when using far-away devices in the network, apart from the added latency brought by longer transmission time.
\bigskip

In the end, the biggest hit to performance comes from doing network traversal and routing. There is a trade-off between latency and bandwidth and not all applications will natively be able to benefit without request distribution and control of some kind. For the scalability of a networked interface to be generally beneficial, a low-latency system is critical.