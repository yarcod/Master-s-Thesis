\chapter{Introduction} \label{intro}
The speed of computer memories is not increasing at the same rate as that of processors. There are applications which does not work optimally with today's locality-focused memory hierarchy, e.g. scientific computing, and this gives rise to noticeable performance losses. This gap in performance has been known for quite some time, and is commonly referred to as the memory wall \cite{wulf1995hitting}, but there is still no one solution for the problem. One of the reasons for this is the extensive use of Double Data Rate (DDR) Dynamic Random Access Memory (DRAM), which locks manufacturers and developers to a very specific memory interface \cite{standard2008double}, and deviating from the DDRx standard could prohibit extensive adoption. Being limited by current memory standards means that these must either be changed dramatically -- not just in an evolutionary way, as is already done with new versions of DDR -- or memories must be rethought all together. 

\section{Non-Volatile RAM (NVRAM)}
There have been a number of attempts to decrease the dependence of DDR DRAM and introduce a type of Non-Volatile RAM (NVRAM). This could both replace the need for a secondary storage system or be used in place of DRAM, in order to thus reduce complexity, energy usage and cost. Since DRAM energy usage equal to more than 20\% of a system's total energy, finding a substitute without sacrificing performance has been a hot research topic\cite{4658649}. Solid State Drives (SSDs) have been available for quite some time, but the underlying technology -- NAND flash memory -- is limited, e.g., by its access time being several orders of magnitude slower than DRAM and that NAND cells ability to hold data deteriorates with each write. A multitude of alternative Non-Volatile Memories (NVM) have been presented; two with a lot of potential and research support being: Phase Change Memory (PCM) and Spin-Transfer Torque RAM (STT-RAM). The former has higher density, i.e. effectively decreasing cost per byte, but is about 2-4 and 10-100 times slower when reading and writing, respectively \cite{Qureshi:2009:SHP:1555754.1555760}\cite{5388621}. In addition, PCM on its own uses more power and the storage cells degrade for every write operation, which lowers their expected lifetime.  Meanwhile, STT-RAM matches DRAM in density and read performance (access time and energy use) but is outperformed when writing \cite{6557176}\cite{6027811}. STT-RAM also needs to be designed with a trade-off between read and write performance \cite{Wang_2013}\cite{Khvalkovskiy_2013}.
\bigskip

The shortcomings of PCM and STT-RAM can to some extent be overcome by adapting memory systems to their respective strong properties. PCM is situated to replace DRAM as main memory because of its potential of lower cost and increased density, while STT-RAM mostly is investigated to be used instead of SRAM in system caches because of its speed and endurance. However, neither technology is truly ready to replace current product lines. Because of the higher costs involved with both the manufacturing and integration as well as the development time needed to fully utilise their respective good properties, there is still some time until they will reach market. One exception, however, is Intel who has launched 3D-Xpoint as a commercial NVRAM alternative (branded as Optane), which, according to the hardware community, is based on PCM \cite{jeongdong_2017}.

\section{3D stacking}
Another proposed solution is stacking current chips on top of one another, and while the idea has been around for some time \cite{lee2000three}\cite{jacob2005predicting}\cite{black2006stacking}, it has until recently not been technically possible to viably manufacture such devices; one of the big hurdles has been the thermal issues stacking entails \cite{5074080}. Stacking memory chips has grown in popularity as one possible solution to today's memory limitations. In theory, stacking can enable a larger amount of memory and computation units being fitted onto the same area, while also allowing higher bandwidths and increased energy efficiency\cite{Lee:2016:SMA:2836331.2832911}. Being designed to be integrated close to the processor, there will be shorter data paths to traverse when memory is accessed, which in turn means less energy spent on large off-chip buses and shorter communication time between processor and memory. However, having memories in three dimensions requires more global bitlines and global sense amplifiers, which, without care, will greatly increase the memory's power usage.
\bigskip

There are several ways to build and integrate stacked memories, 3D IC stacking or 2.5D stacking \cite{6248905}\cite{5702702}\cite{6248842}. Marinissen et al also detail a work-in-progress IEEE standard which additionally defines 5.5D stacking \cite{7519330}. These systems are generally referred to as System in Package (SiP). 2.5D stacking means that chips are put on top of a (passive) silicon interposer, as can be seen in figure \ref{2D-chip}. The connections which are used both within the interposer to connect the chips and between the stacked chips are denoted as Through Silicon Via (TSV) \cite{Lau2018}\cite{lau2012through}. Between each die there are also micro-bumps (<25 Âµm) to create the bond between them. While 3D IC integration also leverages TSV and micro-bumps, it provides a much smaller footprint as well as an (theoretical) order of magnitude higher efficiency, e.g., in terms of link power bandwidth per mm$^{2}$ \cite{6248968}. In addition, it is possible to manufacture stacked chips without micro-bumps and instead only rely on TSV, known as 3D Silicon Integration. The most widely accepted way of doing 3D Si integration is by stacking wafers on top of one another, with the drawback that good dies easily are stacked with bad dies, resulting in a non-functioning chip, i.e., the yields are very low. 3D Si integration has even better potential than 3D IC integration, and there are some proposed methods to alleviate this issue. One being wafer matching, where tested wafers are matched together based on the number of known good dies that would be put together, and another where individual good dies are placed on top of the wafer \cite{Taouil:2010:TCA:1931472.1931973}. There is as always the trade-off between manufacturability and cost, where sufficient and thorough testing can help predict which method is most suitable.

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{figure/2D-integration.png}
\caption{Illustration of a 2.5D chip. The interposer is needed to enable communications between the Logic and DRAM. }
\label{2D-chip}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{figure/3D-integration.png}
\caption{Illustration of a 3D chip. All dies are directly connected through TSV.}
\label{3D-chip}
\end{figure}

Apart from introducing great potential in the future there are also some additional problems with stacking chips, e.g. verification and diverting thermal energy. There are several proposed methods of testing, where the underlying architecture could support both pre-bonding, mid-bonding and post-bonding tests. Pre-bonding tests are performed before and/or after doing wafer thinning, before dies are being stacked. When stacking, tests can be done on partial stacks, called mid-bond test, and complete stacks, called post-bond test. Enabling testing after initial bonding could include creating additional test pins to form a Test Access Port \cite{Marinissen:2012:CES:2492708.2493023}. Alternatively creating special probe pads, due to the difficulties in probing micro-bumps without risking damaging them \cite{5751450}. Furthermore, high performance chips will inevitably lead to heat generation and by putting them  on top of one another the heat cannot easily dissipate; since the space between the dies are too small for air or liquid to pass efficiently, there need to be some other thermal cooling mechanism \cite{5501261}. The optimal thermal scenario is when the hottest chip is placed on top, nearest a heat spreader, which is inherently true when doing 2.5D integration. Having 3D chips makes it more complicated, but separating the most heat generating dies as much as possible minimizes in-package thermal transfer. This is one of several issues that has made 2.5D more realisable today, and still keeps 3D IC integration as a future goal. Due to the many advantages, performance-wise, with 3D stacking, it is very desirable to find a solution to this. Lastly, 3D stacking, compared to 2.5D, is also cheaper to manufacture because the package is smaller and less complex \cite{6263032}. This cost gap could, however, be lowered if the interposer die were to be tested before stacking, which could lead to an increased yield \cite{6542130}.


%%%%%%%%%%%%%%% STACKING MEMORIES %%%%%%%%%%%%%%%
\section{Stacking memories}
While stacking processing units is possible, they are often also the component generating the most heat in a system, and some benefits could be gained from stacking memory chips only. Having a stacked memory will by itself enable it to be denser and allow for more memory on the same die area. In addition, by having memory in multiple layers, it is also possible to place a separate memory controller in the memory stack. This gives the memory controller very close access to its resources and could allow for finer grained control over accesses and atomic memory operations ensured by the memory controller. This could off-load the CPU memory controller -- if one is needed at all!
\bigskip

There are a few noteworthy types of stacked memories: Hybrid Memory Cube (HMC), designed by Micron; High Bandwidth Memory (HBM), a JEDEC standard since 2013; and Wide-IO (WIO), also a JEDEC standard since 2011. While the two former are designed for 2.5D integration -- memory on the same substrate as the processor -- the latter supports 2.5D, but is primarily aimed for 3D integration. These three memory technologies are all based on conventional DRAM chips being stacked, but it could also be possible to stack other, emerging technologies as well. Wu et al. simulated full systems with today's SRAM cache setup and compared that to stacks of PCM and STT-RAM where the architecture took advantage of respective type's strengths, and their results showed notable IPC increases and energy use reduction with this setup \cite{Wu:2009:HCA:1555754.1555761}. More close at hand are the stacked memory cells in SSDs being produced, where the densest flash drives currently use 96 layers of NAND cells, commonly referred to as Vertical NAND (V-NAND) or 3D NAND \cite{tallis_2017}.
\bigskip

These memory technologies are all quite similar, but they primarily target different market segments. HMC is designed for high bandwidth at relatively high energy consumption and is comparatively expensive. This makes it more suitable for High Performance Computing (HPC) systems. HBM was designed specifically for graphics applications, and there have been multiple commercial products (graphics cards) utilising this memory type in recent years. Similarly to HMC, this technology is made for systems where adequate cooling is available and where a higher cost can be somewhat overlooked. WIO is, as opposed to the others, architectured for being integrated on top of System on Chips (SoCs). These types of systems are generally found in mobile devices, which is also what WIO is designed for. Like HBM, WIO hopes to increase the performance of the graphics processors, but inside mobile devices. These specific memory types are talked about in more detail in chapter \ref{background}.
\bigskip

Having memory far away running at high speeds will incur a great cost to power usage, and as such the most preferred way is to have memory as close to the processing unit as possible, but there is a BoM and manufacturing cost of having an interposer. However, many high performance x86 designs today use "chiplet" or Package-on-Package (PoP) designs, where modules, or clusters, of cores and caches are mounted onto an interposer and communicating over a high speed, extensible bus interface. By using this approach, some of the added cost of a larger interposer is already accounted for and should make it a little less costly to integrate stacked memories. There is, however, still no coming around the fact the tried and tested method of using DRAM DIMMs is still cheaper, but with the right workload the added performance could be worth the cost.
\bigskip

The tantalising option of increased bandwidth and potentially lower latency, still has downsides apart from cost. Having stacked memory chips will close in the thermal energy generated and there is no way to easily dissipate it. Furthermore, (especially) stacked memories exhibit a positive feedback loop with respect to power usage, temperature and leakage power. When temperature rises, the DRAM leakage power will increase as well which will increase overall power consumption. A higher power consumption will create even more heat, and it might soon get out of hand \cite{4212027}. There are however suggested ways to mitigate this to some extent by making sure that allocations and accesses are spread out as much as possible inside the cube, thus taking the cube temperature into consideration when placing data \cite{7252085}.
\bigskip

While these emerging types of memories are of interest on their own, whether they are non-volatile or standard DRAM, they could also be used in a Multi-Level Memory (MLM) design. An MLM design could utilise both the higher bandwidth of, e.g., HMC with the lower cost of DDR memory. A method has been described by Jayaraj et al \cite{Jayaraj:2015:PPM:2818950.2818976}, where HMC is being used as a scratchpad type memory while DDR is still being used as main memory. This results in a higher performance while at a lower Bill of Materials (BoM) cost compared to a system using only DDR. However, the added cost of designing such a system is not included and not all applications are memory bound enough to benefit. Furthermore, it would also be necessary for software developers to optimise their applications for this setup in order to see any major gains \cite{Bender:2015:KCT:2818950.2818977}\cite{BENDER2017213}. Lastly, it could be possible to use a hybrid-type memory using PCM memory with a DRAM buffer to hide or minimise the shortcomings of PCM, i.e. decrease latencies and write energy consumption \cite{Lee:2009:APC:1555815.1555758}.
\bigskip

TODO: Perhaps include a small conclusion that today the best use of stacked memories could be in combination with more traditional DRAM, in order to have a trade-off between price, capacity and price. Pure stacked solutions tend to get expensive and hard(er) to control thermals. AMD skipping on HBM recently. Ref to interview where executives compare GDDR and HBM memories. additionally the NVRAM memories are still some time in the future with only Intel Optane ready for real-world use so far (PCM); they may potentially provide the ultimate storage solutions, but not yet. 


%%%%%%%%%%%%%%%%%%%%%%%%%%% Goal and Limitations %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goal}
Novel technologies often come with a lot of promise -- in this case to breach or lower the so far insurmountable memory wall -- but the statistics and numbers presented are usually under ideal circumstances \cite{Radulovic:2015:TWM:2818950.2818955}. The potential for lower memory latency is for certain applications very welcome, but this has not been extensively tested for these emerging memory types. This thesis aims to investigate how big an impact the increased latency of having multiple Hybrid Memory Cubes in a network has on performance when running applications. The applications used will primarily be from the well known SPEC2006 suite. This will be achieved by running x86 benchmark binaries through an HMC simulator along with a CPU architectural simulator, HMCSim and SST respectively. 

\section{Limitations}
Stacked memories aim at providing higher bandwidth along with lower memory latency. As such, the applications expected to be affected the most by this are memory bound and those will be run primarily. However, since general application performance is also important, a few of these will be included for reference as well (TODO: ?). The performance of HMC itself has already been discussed and how it compares to common DDR memory, and this has already been tested by others, e.g., Santos et al \cite{santosHMCvsDRAM2017}. The goal here is simply to measure the relative performance between HMC network setups. In order to narrow down the worst-case performance impact, only a chain type topology will be investigated. This is both due to lack of time and due to the extensibility of the topology implementation made this difficult.
\bigskip

For good performance using this kind of memory system, a Non-Uniform Memory System (NUMA), the OS needs to be aware of this and make memory allocations based on this fact, i.e., allocate on memory banks that are closest to the current processor or core. The run benchmarks do not use enough memory to fill one cube, thus needing to use the next cube in the network, and because of that memory is only allocated on one cube at a time for each run. Data could have been allocated randomly over multiple, or all, cubes, but this would create unrealistic scenarios and totally disregard the use of spatial locality. Although HMC uses a closed page policy, the fact that a two pages for the same application could be spread out on devices several hops apart would both induce hopelessly poor performance as well as indeterministic results.
\bigskip

As has been described previously, stacked memories can both see performance benefits as well as increased power efficiency by allocating data in a manner that spreads thermals evenly in the stack. This is out of the scope of this thesis, and no regard of this potential performance degradation is taken into account. The complete system is assumed to have uniform performance across all runs of applications. In addition, no special care has been taken to create a good CPU architecture in the SST simulator. The only thing that has been configured as realistically as possible are the cache sizes -- this is relevant since we want applications to hit main memory as often as they would on similar systems.

%In the following chapters, we start off with the background of today's SDRAM memories and continue to explain how the emerging technologies operate. Then, we describe the different components, benchmark applications and simulators used in achieving the subsequent results. After that, we discuss the results and the implications they might have and finally give our conclusion. 