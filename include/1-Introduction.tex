\chapter{Introduction} \label{intro}
The speed of computer memories is not increasing at the same rate as that of processors. There are applications which does not work optimally with today's locality-focused memory hierarchy, e.g. scientific computing, and this gives rise to noticeable performance losses. This gap in performance has been known for quite some time, and is commonly referred to as the memory wall \cite{wulf1995hitting}, but there is still no one solution for the problem. One of the reasons for this is the extensive use of Double Data Rate (DDR) Dynamic Random Access Memory (DRAM), which locks manufacturers and developers to a very specific memory interface \cite{standard2008double}, and deviating from the DDRx standard could prohibit extensive adoption. Being limited by current memory standards means that these must either be changed dramatically -- not just in an evolutionary way, as is already done with new versions of DDR -- or memories must be rethought all together.
\bigskip

Because the memory wall has been in place for centuries, computer architects have been forced to find ways to hide latencies from applications as much as possible. This is often done by using multiple levels of cache, which will store interim copies of data until written to the main memory. In the absence of a technology with better latency, the trend has been more towards getting as much data from point A to point B as possible, i.e. increasing the bandwidth. Using stacked memories, however, systems could potentially benefit from both higher bandwidths as well as lower latencies because of their possible placement close to the processor. TODO: Define a/the potentials of having multiple memory devices in a network. 
\bigskip

Using the unique features of HMC, presented in the next chapter, where a packet-based, high speed interface with a more abstract protocol could enable for a more flexible, scalable and performant solution in the end. Furthermore, HMC provides support for routing topologies in its network design which could lead to simpler systems in datacentre-like systems while potentially being scalable down for more regular usage as well. Finally, some of its features have not undergone extensive testing, one being HMC's networking capabilities when having multiple nodes in such a network, and how this affects performance of an application running on a host. With this we set up a network situation with several memory nodes and investigate how applications fare while running on close and far memory.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Goal and Limitations %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goal}
Novel technologies often come with a lot of promise -- in this case to breach or lower the so far insurmountable memory wall -- but the statistics and numbers presented are usually under ideal circumstances \cite{Radulovic:2015:TWM:2818950.2818955}. The potential for lower memory latency is for certain applications very welcome, but this has not been extensively tested for these emerging memory types. This thesis aims to investigate how big an impact the increased latency of having multiple Hybrid Memory Cubes in a network has on performance when running applications. The applications used will primarily be from the well known SPEC2006 suite. This will be achieved by running x86 benchmark binaries through an HMC simulator along with a CPU architectural simulator, HMCSim and SST respectively. 

\section{Limitations}
Stacked memories aim at providing higher bandwidth along with lower memory latency. As such, the applications expected to be affected the most by this are memory bound and those will be run primarily. However, since general application performance is also important, a few of these will be included for reference as well (TODO: ?). The performance of HMC itself has already been discussed and how it compares to common DDR memory, and this has already been tested by others, e.g., Santos et al \cite{santosHMCvsDRAM2017}. The goal here is simply to measure the relative performance between HMC network setups. In order to narrow down the worst-case performance impact, only a chain type topology will be investigated. This is both due to lack of time and due to the extensibility of the topology implementation made this difficult.
\bigskip

For good performance using this kind of memory system, a Non-Uniform Memory System (NUMA), the OS needs to be aware of this and make memory allocations based on this fact, i.e., allocate on memory banks that are closest to the current processor or core. The run benchmarks do not use enough memory to fill one cube, thus needing to use the next cube in the network, and because of that memory is only allocated on one cube at a time for each run. Data could have been allocated randomly over multiple, or all, cubes, but this would create unrealistic scenarios and totally disregard the use of spatial locality. Although HMC uses a closed page policy, the fact that a two pages for the same application could be spread out on devices several hops apart would both induce hopelessly poor performance as well as indeterministic results.
\bigskip

As has been described previously, stacked memories can both see performance benefits as well as increased power efficiency by allocating data in a manner that spreads thermals evenly in the stack. This is out of the scope of this thesis, and no regard of this potential performance degradation is taken into account. The complete system is assumed to have uniform performance across all runs of applications. In addition, no special care has been taken to create a good CPU architecture in the SST simulator. The only thing that has been configured as realistically as possible are the cache sizes -- this is relevant since we want applications to hit main memory as often as they would on real systems.

%In the following chapters, we start off with the background of today's SDRAM memories and continue to explain how the emerging technologies operate. Then, we describe the different components, benchmark applications and simulators used in achieving the subsequent results. After that, we discuss the results and the implications they might have and finally give our conclusion. 