\chapter{Introduction}
The speed of computer memories is not increasing at the same rate as that of processors. There are applications which does not work optimally with today's locality-focused memory hierarchy, e.g. scientific computing, and this gives rise to noticeable performance losses. This gap in performance has been known for quite some time, and is commonly referred to as the memory wall \cite{wulf1995hitting}, but there is still no one solution for the problem. One of the reasons for this is the extensive use of Double Data Rate (DDR) Dynamic Random Access Memory (DRAM), which locks manufacturers and developers to a very specific memory interface \cite{standard2008double}, and deviating from the DDRx standard could prohibit extensive adoption. Being limited by current memory standards means that these must either be changed dramatically -- not just in an evolutionary way, as is already done with new versions of DDR -- or memories must be rethought all together. 
\bigskip

There have been a number of attempts to decrease the dependence of DDR DRAM and introduce a type of Non-Volatile RAM (NVRAM). This could both replace the need for a secondary storage system or be used in place of DRAM, in order to thus reduce complexity, energy usage and cost. Since DRAM energy usage equal to more than 20\% of a system's total energy, finding a substitute without sacrificing performance has been a hot research topic\cite{4658649}. Solid State Drives (SSDs) have been available for quite some time, but the underlying technology -- NAND flash memory -- is limited, e.g., by its access time being several orders of magnitude slower than DRAM and that NAND cells ability to hold data deteriorates with each write. A multitude of alternative Non-Volatile Memories (NVM) have been presented; two with a lot of potential and research support being: Phase Change Memory (PCM) and Spin-Transfer Torque RAM (STT-RAM). The former has higher density, i.e. effectively decreasing cost per byte, but is about 2-4 and 10-100 times slower when reading and writing, respectively \cite{Qureshi:2009:SHP:1555754.1555760}\cite{5388621}. In addition, PCM on its own uses more power and the storage cells degrade for every write operation, which lowers their expected lifetime.  Meanwhile, STT-RAM matches DRAM in density and read performance (access time and energy use) but is outperformed when writing \cite{6557176}\cite{6027811}. STT-RAM also needs to be designed with a trade-off between read and write performance \cite{Wang_2013}\cite{Khvalkovskiy_2013}.
\bigskip

The shortcomings of PCM and STT-RAM can to some extent be overcome by adapting memory systems to their respective strong properties. PCM is situated to replace DRAM as main memory because of its potential of lower cost, 
Research regarding STT-RAM's usage has primarily been targeting replacing SRAM inside system caches. 
TODO: Talk about why these have not made it to market yet, and why it is still interesting to search for options.
\bigskip

Another proposed solution is stacking memory chips on top of one another, and the idea has been around for some time \cite{lee2000three}\cite{jacob2005predicting}\cite{black2006stacking}, but it has until recently not been technically possible to viably manufacture such devices; one of the big hurdles has been the thermal issues stacking entails \cite{5074080}. Stacking memory chips has grown in popularity as one possible solution to today's memory limitations. In theory, stacking can enable a larger amount of memory being fitted onto the same area, while also allowing higher bandwidths and increased energy efficiency\cite{Lee:2016:SMA:2836331.2832911}. Being designed to be integrated close to the processor, there will be shorter data paths to traverse when memory is accessed, which in turn means less energy spent on large off-chip buses and shorter communication time between processor and memory. However, having memories in three dimensions requires more global bitlines and global sense amplifiers, which, without care, will greatly increase the memory's power usage.
\bigskip

There are several ways to build and integrate stacked memories, 3D IC stacking or 2.5D stacking \cite{6248905}\cite{5702702}\cite{6248842}. Marinissen et al also details a work-in-progress IEEE standard which additionally defines 5.5D stacking \cite{7519330}. 2.5D stacking means that chips are put on top of a (passive) silicon interposer -- which contains the connections between the stacked chips -- as can be seen in figure \ref{2D-chip}. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{figure/2D-integration.png}
\caption{Illustration of what a 2.5D chip might look like. The interposer is needed to enable communications between the Logic and the DRAM. }
\label{2D-chip}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.75\linewidth]{figure/3D-integration.png}
\caption{Illustration of what a 3D chip might look like. Unlike 2.5D integration, it does not need an interposer to do the data routing, as all dies are directly connected. }
\label{3D-chip}
\end{figure}

Apart from introducing great potential in the future there are also some additional problems with stacking chips, e.g. testing and diverting thermal energy. There are several proposed ideas of how to test 
3D vs 2.5D. Testing challenges. Cost? Thermal issues - how to move heat out of the middle of the System-in-Package (SiP)?
Passive interposer = 2.5D -> Easier to control thermals. 5,5D proposed name (IEEE) for designs with multiple towers on package. 3D IC integration is something like stacking many/two 2.5 integrations on top of another, where the high performance/hot chips are facing upwards in order to connect to a thermal heat spreader and heat sink. TODO: Mention maturity, where 2.5D is realisable today.
TODO: TALK MORE ABOUT INTEGRATION TECHNIQUES AND PROVIDE A FEW INTERESTING ARTICLES ON THE SUBJECT.
\bigskip

There are a few noteworthy types of stacked memories: Hybrid Memory Cube (HMC), designed by Micron; High Bandwidth Memory (HBM), a JEDEC standard since 2013; and Wide-IO (WIO), also a JEDEC standard, since 2011. While the two former are designed for 2.5D integration -- memory on the same substrate as the processor -- the latter supports 2.5D, but is primarily aimed for 3D integration. TODO: Cite article stacking MRAM/STT-RAM as well to show that there are alternatives out there. TODO: Also mention that modern NAND flash today is stacked (3D- or V-NAND), used a lot. 
\bigskip

These memory technologies are all quite similar, but they primarily target different market segments. HMC is designed for high bandwidth at relatively high energy consumption and is comparatively expensive. This makes it more most suitable for High Performance Computing (HPC) systems. HBM was designed specifically for graphics applications, and there have been multiple commercial products (graphics cards) utilising this memory type in recent years. Similarly to HMC, this technology is made for systems where adequate cooling is available and where a higher cost can be somewhat overlooked. WIO on the other hand is architectured for being integrated on top of -- 3D integrated -- System on Chips (SoCs). These types of systems are generally found in mobile devices, which is also what WIO is designed for. Like HBM, WIO hopes to increase the performance of the graphics processors inside mobile devices. 
\bigskip

While these emerging types of memories are of interest on their own, whether they are non-volatile or standard DRAM, they could also be used in a Multi-Level Memory (MLM) design. An MLM design could utilise both the higher bandwidth of, e.g., HMC with the lower cost of DDR memory. A method has been described by Jayaraj et al. \cite{Jayaraj:2015:PPM:2818950.2818976}, where HMC is being used as a scratchpad type memory while DDR is still being used as main memory. This results in a higher performance while at a lower Bill of Materials (BoM) cost compared to a system using only DDR. However, the added cost of designing such a system is not included and not all applications are memory bound enough to benefit. Furthermore, it would also be necessary for software developers to optimise their applications for this setup in order to see any major gains \cite{Bender:2015:KCT:2818950.2818977}\cite{BENDER2017213}. TODO: If including PCM, mention multi-level memories here as well, with examples using PCM and DRAM buffers. Aka hybrid memory types.
\bigskip

%In the following chapters, we start off with the background of today's SDRAM memories and continue to explain how the emerging technologies operate. Then, we describe the different components, benchmark applications and simulators used in achieving the subsequent results. After that, we discuss the results and the implications they might have and finally give our conclusion. 

\section{Goal}
Novel technologies often come with a lot of promise -- in this case to breach the so far insurmountable memory wall -- but the statistics and numbers presented are usually under ideal circumstances. The potential for lower memory latency is for certain applications very welcome, but this has not been extensively tested. This thesis aims to investigate how big an impact the increased latency of having multiple Hybrid Memory Cubes in a network has on performance on running applications. The applications used will primarily be targeting High Performance Computing (HPC), as that is the main target group of HMC. This will be achieved by running x86 benchmark binaries through an HMC simulator along with a CPU architectural simulator. TODO: expand on why HMC was chosen in the end. E.g. there are actual devices to by and test, available simulator models etc. 

\section{Limitations}
Stacked memories aim at providing higher bandwidth along with lower memory latency. As such, the applications expected to be affected the most by this are memory bound and those will be run primarily. However, since general application performance is also important, a few of these will be included for reference as well. TODO: Relative performance in terms of runtime basically. Compute IPC possible? 
