\chapter{Introduction} \label{intro}
The speed of computer memories is not increasing at the same rate as that of processors. There are applications which do not work optimally with today's locality-focused memory hierarchy, i.e. memory bound applications which core data does not fit inside caches, and this gives rise to noticeable performance losses. This gap in performance has been known for quite some time, and is commonly referred to as the memory wall \cite{wulf1995hitting}, but there is still no one solution for the problem. One contributing factor is the extensive use of Double Data Rate (DDR) \cite{standard2008double} Dynamic Random Access Memory (DRAM), which locks manufacturers and developers to a very specific memory interface, and deviating from the DDRx standard could prohibit extensive adoption. While DDR does provide both good latency as well as higher bandwidth with each generation, it is based on the same technology which was created more than 20 years ago. This standard prohibits custom solutions and innovations where such could have been beneficial, either to latency, bandwidth or power. Additionally, there is a trade-off between speed and power due to the bus used to connect DIMMs to the CPU, where such off-chip connections use a lot more power than on-chip and the trade-off made often leaves bandwidth comparably low. Being limited by current memory standards means that these must either be changed dramatically -- not just in an evolutionary way, as is already done with new versions of DDR -- or memories must be rethought all together.
\bigskip

Because the memory wall has been in place for decades, computer architects have been forced to find ways to hide latencies from applications as much as possible. This is often done by using multiple levels of cache, which will store interim copies of data until written to the main memory. In the absence of a technology with better latency the trend has been more towards getting as much data from point A to point B as possible, i.e. increasing the bandwidth. One way to benefit from both potentially higher bandwidth as well as lower latencies is to use stacked memories, because of their possible placement close to the processor. Having a close proximity to the processor means that there is less of a trade-off between bus power and memory speed, while a stacked memory also ensures that density is comparable to that of ordinary DDR. In addition, the smaller distances and compact layout of stacked memory can enable lower latencies.
\bigskip

Hybrid Memory Cube (HMC), presented in the next chapter, uses a packet-based, high speed interface with an abstract protocol which could enable more flexible, scalable and better performing memory solutions. Furthermore, HMC provides support for routing topologies between links and memory nodes, which makes it possible to add memory and computation units anywhere in an existing network. This could be advantageous in datacentre-like systems, where large networks of computation elements are used and changed often, memory requirements for applications are huge, and by using memory networks, resource utilisation and distribution could be simplified. Finally, some of its features have not undergone extensive testing, one being HMC's networking capabilities when having multiple nodes in such a network, and how this affects performance of an application running on a host.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Goal and Limitations %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goal}
Novel technologies often come with a lot of promise -- in this case to breach or lower the so far insurmountable memory wall -- but the statistics and numbers presented are usually under ideal circumstances \cite{Radulovic:2015:TWM:2818950.2818955}. The potential for lower memory latency is for certain applications very welcome, but this has not been extensively tested for these emerging memory types. There have, however, been studies of how HMC perform compared to DDR memory using equivalent setups, based on the limitations set by DDR \cite{santosHMCvsDRAM2017}. As opposed to DDR, HMC can be placed in a network which makes direct comparisons using these different approaches unsubstantiated. This thesis aims to investigate what impact having multiple HMCs in a network has on applications' performance. In the simulated environment we will target general purpose applications in order to get an as broad a picture as possible. This will be achieved by running these applications through an HMC simulator along with a CPU architectural simulator. 

\section{Limitations}
Stacked memories aim at providing higher bandwidth along with lower memory latency. As such, the applications expected to be affected the most by this network layout are memory bound and those will be prioritised. However, since general application performance is also important, a few of these will be included. This limitation is set because simulations take a long time to complete and a limited number can be run simultaneously. In order to narrow down the worst-case performance impact, only a chain type topology will be investigated. This is both due to lack of time and due to the topology implementation's lack of extensibility. Lastly, stacked memories provide performance benefits as well as increased power efficiency by allocating data in a manner that spreads thermals evenly in the stack. This is out of the scope of this thesis and no regard of this potential performance degradation is taken into account.
\bigskip

In order to achieve competitive performance on Non-Uniform Memory Access (NUMA) systems, i.e. our setup, the operating system needs to be aware of this and allocate on memory banks that are virtually closest to the current processor or core. The run benchmarks do not use enough memory to fill one cube, thereby needing to use the next cube in the network, and because of that memory is only allocated on one cube at a time for each run. Data could also have been allocated randomly over multiple, or all, cubes, in order to mimic Linux's "interleaved" scheduling policy, but this effort eventually failed. Although an interesting test, the fact that a two pages for the same application could be spread out randomly on devices several hops apart could give very indeterministic results as well as too varying simulation times.
\bigskip

\section{Thesis layout}
In the following chapters, we start off with a background describing the fundamentals of stacking chips and memories. Following this, we talk about today's SDRAM memories work and continue to explain how a few emerging technologies operate and compare. Then we describe the different simulators used, the needed changes made therein, and what benchmark applications are used in achieving the subsequent results. The results are summarised and we discuss their implications. Finally we present our conclusion from this work.