\chapter{Introduction} \label{intro}
The speed of computer memories is not increasing at the same rate as that of processors. There are applications which do not work optimally with today's locality-focused memory hierarchy, i.e. memory bound applications which core data does not fit in the cache, and this gives rise to noticeable performance losses. This gap in performance has been known for quite some time, and is commonly referred to as the memory wall \cite{wulf1995hitting}, but there is still no one solution for the problem. One of the reasons for this is the extensive use of Double Data Rate (DDR) Dynamic Random Access Memory (DRAM), which locks manufacturers and developers to a very specific memory interface \cite{standard2008double}, and deviating from the DDRx standard could prohibit extensive adoption. While DDR does provide both good latency as well as higher bandwidth with each generation, it is based on the same technology which was created more than 20 years ago. This standard prohibits custom solutions and innovations where such could have been beneficial, either to latency, bandwidth or power. Additionally, there is a trade-off between speed and power due to the bus used to connect DIMMs to the CPU, where such off-chip connections use a lot more power than on-chip and the trade-off often leaves bandwidth comparably low. Being limited by current memory standards means that these must either be changed dramatically -- not just in an evolutionary way, as is already done with new versions of DDR -- or memories must be rethought all together.
\bigskip

Because the memory wall has been in place for decades, computer architects have been forced to find ways to hide latencies from applications as much as possible. This is often done by using multiple levels of cache, which will store interim copies of data until written to the main memory. In the absence of a technology with better latency, the trend has been more towards getting as much data from point A to point B as possible, i.e. increasing the bandwidth. One way to benefit from both potentially higher bandwith as well as lower latencies is to use stacked memories, because of their possible placement close to the processor. Having a close proximity to the processor means that there is less of a trade-off between bus power and memory speed, while a stacked memory also ensures that density is comparable to that of ordinary DDR. In addition, the smaller distances and compact layout of stacked memory can enable lower latencies.
\bigskip

Hybrid Memory Cube (HMC), presented in the next chapter, uses a packet-based, high speed interface with an abstract protocol which could enable more flexible, scalable and better performing memory solutions. Furthermore, HMC provides support for routing topologies between links and memory nodes, which makes it possible to add memory and computation units anywhere in an existing network. This could provide an advantage in datacenter-like systems, where large networks of computation elements are used and changed often, and by using memory networks, resource utilisation and distribution could be simplified. Finally, some of its features have not undergone extensive testing, one being HMC's networking capabilities when having multiple nodes in such a network, and how this affects performance of an application running on a host.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Goal and Limitations %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goal}
Novel technologies often come with a lot of promise -- in this case to breach or lower the so far insurmountable memory wall -- but the statistics and numbers presented are usually under ideal circumstances \cite{Radulovic:2015:TWM:2818950.2818955}. The potential for lower memory latency is for certain applications very welcome, but this has not been extensively tested for these emerging memory types. There have, however, been studies of how HMC perform compared to DDR memory using equivalent setups, based on the limitations set by DDR \cite{santosHMCvsDRAM2017}. As opposed to DDR, HMC can be placed in a network which makes direct comparisons using these different approaches unsubstantiated. This thesis aims to investigate what impact having multiple HMCs in a network has on performance when running applications, since latency will be increased for every added node. In the simulated environment we will target general purpose applications in order to get an as broad a picture as possible. This will be achieved by running these applications through an HMC simulator along with a CPU architectural simulator. 

\section{Limitations}
Stacked memories aim at providing higher bandwidth along with lower memory latency. As such, the applications expected to be affected the most by this network layout are memory bound and those will be prioritised. However, since general application performance is also important, a few of these will be included. This limitation is mainly in place because simulations take a long time to complete and not too many can be run simultaneously.  In order to narrow down the worst-case performance impact, only a chain type topology will be investigated. This is both due to lack of time and due to the extensibility of the topology implementation made this difficult. Lastly, stacked memories can both see performance benefits as well as increased power efficiency by allocating data in a manner that spreads thermals evenly in the stack. This is out of the scope of this thesis, and no regard of this potential performance degradation is taken into account
\bigskip

For good performance using this kind of memory system (Non-Uniform Memory Access (NUMA)), the OS needs to be aware of this and make memory allocations based on this fact, i.e., allocate on memory banks that are closest to the current processor or core. The run benchmarks do not use enough memory to fill one cube, thus needing to use the next cube in the network, and because of that memory is only allocated on one cube at a time for each run. Data could have been allocated randomly over multiple, or all, cubes, but this would create unrealistic scenarios and totally disregard the use of spatial locality. Although HMC uses a closed page policy, the fact that a two pages for the same application could be spread out on devices several hops apart would both induce hopelessly poor performance as well as indeterministic results.
\bigskip

In the following chapters, we start off with the background of today's SDRAM memories and continue to explain how a few emerging technologies operate. Then we describe the different simulators, the needed changes made therein, and what benchmark applications are used in achieving the subsequent results. After that we discuss the results and the implications they might have and finally give our conclusion. 