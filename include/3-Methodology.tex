\chapter{Methodology} \label{method}
Evaluating new memory technologies without having physical access to them can be done by simulating their designs. There is always a measure of uncertainty and error when performing simulations, but it still gives a reasonable estimation of what to expect from the real hardware. Some argue that, e.g., cycle accurate simulators might not be the best approach in system evaluation \cite{weaver2008cycle}, but in this study all memories will be evaluated  using the same methodology with the same level of uncertainty. Thus the relationship between their performance should remain relevant. 
\bigskip

HMC is primarily meant to be put on the same substrate as a processor, but using them in a chain topology is likely not a realistic approach. Although it would be supported by the HMC specification, it would be a hard, expensive and underperforming architecture to create and use. Having multiple network hops will inherently add latency and after a certain number of hops subsequent memory devices should arguably be regarded as far-memory, as opposed to being near-memory. With a setup consisting of memory with different access times, the system is a Non-uniform Memory Access (NUMA) system and an OS should place memory allocations as close as possible. The system could also use a simpler prefetching scheme in order to hide latency for sequential access, but that would also increase network load and possibly contention. In order to simplify this, we take a naive approach and regard all cubes as near-memory and we will only allocate memory on one device at a time. This will keep the results focused on impact of increasing number of hops rather than allocation schemes.
\bigskip

First in this chapter the simulation software for microarchitectures and memory technology is described. Next, a number of benchmark applications that will be run in the simulators are selected. Finally, the process of running applications and gathering results is defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Simulators  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulators}
For simulation of the memory network, two simulators are used: Structural Simulation Toolkit (SST) \cite{rodrigues2011structural} and HMCSim \cite{6969550}. The former is used for accurate microarchitecture simulation, while the latter is used to simulate the HMC memory. There were not many simulators for HMC available when this project started, which led to the use of HMCSim. This simulator in turn was already integrated with SST and this significantly lowered the amount of development needed to start. There has since come up other alternative HMC simulators \cite{7544479, Yang:2018:HCH:3240302.3240319} and there has also been created similar tools to accommodate multiple memory cubes in a network setup \cite{Siegl:2017:BAF:3132402.3132403}. Even so, the flexibility and extensibility of both HMCSim and SST has enabled a good basis for this kind of evaluation.
\bigskip

TODO: Possible subsection with alternative simulators for the same purpose?

\subsection{SST}
SST is an architectural simulator which supports execution of x86 binaries towards different memory back-ends \cite{rodrigues2011structural}. It uses a modular front-end back-end framework, where each part can be replaced independently to model different architectural configurations. This modularity and the simulator's parallel simulation environment is enabled by the use of Message Passing Interface (MPI). This makes all communications event-based, and events are tagged with a delivery time. These events are only sent when needed and no real processing time is unnecessarily spent on future events \cite{11.1093/comjnl/bxr069}. Communication within the simulator uses a network model for routing information between components, which enables arbitrary architectural topologies. In addition, the router model can take network usage and congestion into account and monitor where and how often it occurs as well as if it caused any additional latencies.
\bigskip

Inside SST there are multiple ways of executing applications, where they are run in different ways. One way is to, on beforehand, run the application and save a memory trace of what the program did and then replay that inside the simulator using, e.g., \emph{Prospero}. This has the benefit of faster runtimes inside the simulator, while it will take some time to generate and store the memory trace. Another way is to run the application directly in the simulator using dynamic instrumentation of the programs' operations, which would be performed in the SST component \emph{Ariel}. Ariel, in turn, relies on Intel PIN Tool for running x86 binaries \cite{Luk:2005:PBC:1065010.1065034}. This has the major advantage that mostly any precompiled application can be run immediately through the simulator while also being faster than cycle-accurate models \cite{hammond2018structural}. It also supports thread interaction, but can on the other hand produce results that are not reproducible. Being an easy way to test memory systems, and providing a good trade-off between run-time and preparation time, it became our choice of processor model. It should be noted that SST is stuck on using PIN 2, whereas version 3 has been available for a few years. This is because the new version changes a few fundamental things in its operation, on which SST relies. Despite a collective effort to get support for PIN 3, it would require a major rewrite of the whole component -- which was outside the scope of this work.
\bigskip

The simulator's memory component is called \emph{memHierarchy} and encapsulates both memory controllers, interfaces and technologies. It is designed to be flexible and to easily incorporate new designs without changing too much in the existing logic. The component is also responsible for setting up and handling caches, coherence policies buses. Inside memHierarchy lies a memory controller which in turn is more appropriately named MemBackendConvertor. Here the conversion of application/CPU memory requests into memory backend specific requests are made. The backend is either a built-in memory model or an external tool, which supports, e.g., DRAMSim2 and the aforementioned HMCSim. It is also possible to use multiple backends simultaneously to enable further exploration of hybrid memory types. A memory backend is written specifically for any external memory simulator, and HMCSim uses its own as well.
\bigskip

With integration of HMCSim already present in the simulator, there were few changes needed in order to get a working simulation environment. It has previously been possible to configure multiple memory devices, but it has not had any effect. As such, apart from the need to add support in HMCSim, there also needed to be some adaption to SST. Furthermore, there was no way to address more than one cube from SST and allocations would only have been done to the first device in the topology. After rewriting SST's memory access behaviour it was possible to address all memory devices using unique identifiers. In addition, an attempt was made to spread out data across multiple nodes by allocating data on devices at random virtual addresses. The idea was that since HMC used a closed page policy, it would not matter where data was allocated. However, applications optimised for spatial locality would see enormous performance losses since one piece of its data could be very close while another very far away. Thus it was determined that this way of allocating data would not be viable from a simulation perspective, i.e. taking very long to run. In the end, only one memory unit is being used per simulation.
\bigskip

Because of a limitation in HMCSim, communication to the network is only done using half of the configured links. This was different from the original behaviour, where all links could be used when having a single HMC device. Data is still being sent in parallel in all of the available links as much as possible, but if one link gets full requests will be passed to another link. This is to try avoid bottlenecking memory requests. It would remain theoretically possible to add a processor anywhere in this network, but there has been no check whether SST's coherence mechanisms would still work when this network is used rather than SST's own. 
\bigskip

In addition, SST lacked functionality to start simulating the program execution after a set number of instructions. To stop simulation after a number of instructions was supported from the start, but to start PIN instrumentation after a set number of instructions was not, and that was added. Originally, SST required the application to call a specific function or the simulation had to trace the application from start. All instructions executed before tracing was started were run with little overhead and at relatively high speed.
\bigskip

Lastly, one advantage of using SST when exploring on-chip networks is that arbitrary topologies of compute and memory nodes based on real products can be used. There have, however, been no effort made to match any existing products. As such, memory sizes, latencies and other configurations likely do not correspond to any real device since much of that information is proprietary. The CPU configuration is kept as simple as possible while remaining relatively realistic.

\subsection{HMCSim} \label{HMC-Sim}
HMCSim is, much unlike many other memory simulators available, primarily built with multidimensional memories in mind. This three dimensional approach enables a unique parallel access to banks, which is not accounted for in current memory simulation environments \cite{6969550}. Furthermore, because of HMC's tightly coupled logic die at the base of the memory stack, few other simulators can correctly mimic this integrated memory controller and parallel access scheme. HMCSim implements the HMC specification and enables simulation of HMC devices without physical access to one. It can either function alone from a memory trace or together with an architectural simulator sending memory requests from a running application. The first version of HMCSim implemented the initial versions of HMC, i.e., 1.0 and 1.1, whereas version two featured the additions made to specification 2.0 and 2.1.
\bigskip

HMCSim is designed to support all sorts of topologies in the logic layer, but there will be error response packets sent back if a configuration is unsupported. The user can set up how deep the queues for both the crossbar switch and the vault controller should be as well as measure latencies in clock cycles whenever packets are being delayed in queues. Furthermore, if, e.g., a write operation is sent with an address that is not physically closest to any of the vaults of the link the request happens to be sent through, the added latency of having to switch the packet through the crossbar switch to another vault is accounted for. While there is support for adding more devices and it is \emph{supported} to have them in a chain, the main functionality for adding more devices is connecting them in parallel to the host. This would be analogous to having a multiple memories connected over individual channels. In addition, the simulator supports different clock domains, and thus asynchronous operation, between host and memory devices so that memories, CPUs and SERDES links can use different frequencies. Between version one and two, of both the HMC specification and the simulator, there were a few features added, such as support the new 256 byte read and write operations. Another interesting feature added was Custom Memory Commands (CMC). One example of such a CMC is an atomic memory operation, where the base logic layer handles the atomicity and performs a read-modify-write by itself on its local memory stack \cite{7529923}. This does require application and/or compiler awareness, however, and as such is not used for a non-optimised program.
\bigskip

HMCSim was already integrated as a memory backend in SST and the work to allow communication between a CPU model and the memory simulator was already done \cite{voskuilen2018sst}. The HMCSim integration utilises custom opcodes from SST to send commands to the memory, which also corresponds to how a HMC would work in a real application. While HMCSim has supported configurations of multiple devices, they could not communicate with one another. There was an awareness of other devices, but requests that were destined for another device were turned into zombie requests. The first addition to HMCSim was to support forwarding and routing of packets. The implementation is pretty simple, and seeks primarily to support this specific use case. A communication path was setup between devices that were linked together, and half of the available links were used to forward memory operations to the next device in line. This leaves the last cube in the chain not fully utilising its links, and could be a place to add another host in the network without any additional loss of bandwidth. Furthermore, this effectively means that we have halved the available bandwidth compared to only connecting to one device. Since HMC specification is not very strict on how communication is done within a network this is still an acceptable solution.
\bigskip

TODO: Create a beautiful picture of how different nodes connect to each other, with the zig-zag pattern that was made. This was due to communication needed to be made on the same link as it originated - technical limitation.
\bigskip

Now that there was support for inter-memory communication, there need be a way to set things up as well. The very basic case which is tested in here only allows for devices to be connected in chains. This is to show the worst case scenarios, and how multiple hops in a network will affect performance. A support for topologies was created, although not further explored. Possible obvious extensions to this implementation would be to allow for more topologies, or, more importantly, a way to easily create custom topologies. As mentioned earlier, the new implementation is not very flexible in its way of creating new topologies, as it works by iterating over devices and links in order to get a functioning network. It is however possible, but it was decided that it would be enough to just measure worst-case scenarios.
\bigskip

In order to measure system latencies, the results obtained by Gokhale et al were interpreted as 93 ns with our setup \cite{10.1145/2833179.2833184}. Since we use a memory clock of 1 GHz this becomes 93 clock cycles of waiting. This is set as the base latency when accessing data in a DRAM array. All links are set to have a latency of 9 ns, where the high speed SERDES interface, switching time is taken into account. In addition to these latencies, there is also the routing of request and response data from each link to the corresponding vault where data is stored. A simple round robin scheme is used to access each available link from the host in order to distribute load as evenly as possible. It should be noted that no regard to the feasibility of having this many HMC devices on the same substrate is taken into account; quite possibly would only a subset of all configured memory cubes use the 9 ns latency, while devices farthest away would use an off-chip bus with a higher latency. For simplicity, all links are considered equal.
TODO: Also include HMC config in some kind of table, e.g. link speed, size, banks etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Benchmarks  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarks} \label{method-benches}
In order to evaluate performance of the system, a number of benchmarks from the SPEC CPU2006 will be run \cite{henning2006spec}. Due to the amount of time it takes to simulate these different configurations, a few benchmarks were selected based on their memory intensity. Prakash et al. made a performance characterization for the different test suites in SPEC, and determined that: \emph{mcf}, \emph{milc}, \emph{soplex}, \emph{lbm} and \emph{omnetpp} were the benchmarks that had the most L2 cache misses, which means that they utilised the main memory the most \cite{prakash2008performance}. Similar studies have been made, which mostly come to the same conclusion regarding memory-bound applications \cite{4664856, 4086140, bird2007performance}. Since the goal is to find worst-case performance degradation in an HMC network, we want to use applications which use the memory as much as possible. In addition, to test how a much more CPU-bound application reacts to the added memory latency, we also add \emph{namd} to the list. This should give us a good view of how different kinds of applications work in this setting. 
\bigskip

%A benchmark which measures a system's sustainable memory bandwidth is STREAM \cite{mccalpin1995memory}, which is a memory bound application. It is designed to perform memory operations which exceed the cache size, and thus generates a lot of last level cache (LLC) misses. This kind of utilization will stress the memory at its maximum and, while unrealistic, truly give a worst-case scenario. TODO: Stream not run! Do?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Measurments  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Measurement approach}
We measure latency only in HMCSim and disregard any latency SST might introduce. This also makes gathering results easier, but will, on the other hand, make it impossible to see how long time it took for the execution to actually process the data. From HMCSim we measure how often different latencies occur and count the average latency. The average latency will be used to compare between system configurations, while the distribution of access times will give us an idea of contention in the links and queues. We first perform a test run for a single HMC in order to establish application latency behaviour and sensitivity. Then we run the same benchmarks using three, five and seven devices to see how application performance maps towards latency. Data will always be allocated on the last device in the chain in order to maximise the number of hops, while minimising potential overhead with having more devices active in the simulator than is being used. In order to avoid having to run entire, enormous applications we use a naive approach to limiting simulation time where the first one billion instructions are skipped, i.e. not simulated, and the execution is limited to one billion instructions. In addition, cache warm-up is set to one hundred million instructions. While this naive approach is not ideal, the alternatives required too much additional work to use. One method could have been SimPoint \cite{hamerly2005simpoint}, or rather PinPoint \cite{pinplay2010patil}, which is a version of SimPoints based on the workings of PIN. PinPoints could have been possible to use since PIN is already featured in the Ariel CPU model used in SST, but the replay of the so called Pinballs required changes far greater than what was in the scope for this thesis.
\bigskip

Simulating real applications in virtual hardware without any real interaction is hard and there are many things that can break along the way. The simulations were run on a by Chalmers provided compute cluster and took quite some time to complete. This cluster luckily used an older version of Ubuntu with a relatively old kernel, and since PIN 2 depends on older kernel functionality it meant it could be run without a hitch. In addition, in order to support the old ABI used with some of its binaries while having full support for C++11, GCC 4.9.4 had to be used, specifically. 
\bigskip

The simulations were run with multiple configurations, where every application was configured to be run with either two or four links between each device, enabling between one and seven memory cubes and data was allocated on the last memory unit in the network. The reason for testing both two and four links is to see whether the increased bandwidth, and in this case amount of parallelism, would significantly impact the latencies. Note that while HMC supports four and eight links in the specification, we can only use half of them from the host as the other half is used to interconnect the rest of the network, as explained in \ref{HMC-Sim}. Time of flight is measured from the CPU's point of view, i.e., from when it was first sent from the host to when the response arrived back. Execution time is measured during the time the application is traced in the simulator and is used to measure performance impact.
\bigskip