\chapter{Methodology}
Evaluating new memory technologies without having physical access to them requires simulation of their designs. There is always a measure of uncertainty and error when performing simulations, but it still gives a reasonable estimation of what to expect from the real hardware. Some argue that, e.g., cycle accurate simulators might not be the best approach in system evaluation \cite{weaver2008cycle}, but in this study all memories will be evaluated with the same errors, and thus the relationship between their performances should remain relevant. 
\bigskip

HMC is meant to be used as near memory -- being integrated close to the CPU -- and putting them in a chain might be a stretch. Although it would be both supported and justifiable from a technical point of view, it might be hard and/or expensive to implement this in reality. Having multiple jumps in a network, which inherently will increase latency, should after a certain threshold arguably be regarded as far-memory. This would then imply that we would need to either regard this system as a MLM or a NUMA and then there would need to be policies used for that as well. Alternatively there could be some intelligent prefetching added in the network to hide latency, but that would also increase network load. In this case we will simplify and regard all HMC nodes as near memory. In addition, we will only allocate memory on one cube at a time.
\bigskip

First in this chapter the simulation software for micro architectures and memory technology is described. Next, a number of benchmark applications that will be run in the simulators are selected. Finally, the process of gathering and interpreting results is defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Simulators  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulators}
For simulation of the memory network, two simulators will be used: Structural Simulation Toolkit (SST) and HMCSim. The former is used for accurate microarchitecture simulation, while the latter is used to simulate the HMC memory. There were not many simulators for HMC available when this project started, which lead to the use of HMCSim. This simulator in turn was already integrated with SST and this significantly lowered the amount of development needed to start. There has since come up other alternative HMC simulators \cite{7544479} \cite{Yang:2018:HCH:3240302.3240319} and there has also been created similar tools to accommodate multiple memory cubes in a network setup \cite{Siegl:2017:BAF:3132402.3132403}. Even so, the flexibility and extensibility of both HMCSim and SST has enabled a good basis for this kind of evaluation.
\bigskip

\subsection{SST}
SST is an architectural simulator which supports execution of x86 binaries towards different memory back-ends \cite{rodrigues2011structural}. It uses a modular front-end back-end framework, where each part can be replaced independently to model different architectural configurations. This modularity and the simulator's parallel simulation environment is enabled by the use of Message Passing Interface (MPI). This makes all communications event-based, and events are tagged with a delivery time. These events are only sent when needed and no real processing time is unnecessarily spent on future events \cite{11.1093/comjnl/bxr069}. Communication within the simulator uses a network model for routing information between components, which enables arbitrary architectural topologies. In addition, the router model can take network usage and congestion into account and monitor where and how often it occurs as well as if it caused any additional latencies.
\bigskip

Inside SST there are multiple ways of executing applications, where they are run in different ways. One way is to, on beforehand, run the application and save a memory trace of what the program did and then replay that inside the simulator using, e.g., \emph{Prospero}. This has the benefit of faster runtimes inside the simulator, while it will take some time to generate and store the memory trace. Another way is to run the application directly in the simulator using dynamic instrumentation of the programs' operations, which would be performed in the SST component \emph{Ariel}. Ariel, in turn, relies on Intel PIN Tool for running x86 binaries \cite{Luk:2005:PBC:1065010.1065034}. This has the major advantage that mostly any precompiled application can be run immediately through the simulator while also being faster than cycle-accurate models \cite{hammond2018structural}. It also supports thread interaction, but can on the other hand produce results that are not reproducible. Being an easy way to test memory systems, and providing a good trade-off between run-time and preparation time, it became our choice of processor model. It should be noted that SST is stuck on using PIN 2, whereas version 3 has been available for a few years. This is because the new version changes a few fundamental things in its operation, on which SST has relied. Despite a collective effort to get support for PIN 3, it would require a major rewrite of the whole component -- which was outside the scope of this work.
\bigskip

The simulator's memory component is called \emph{memHierarchy} and encapsulates both memory controllers, interfaces and technologies. It is designed to be flexible and to easily incorporate new designs without changing too much in the existing logic. One example of this flexibility is that it took Voskuilen et al. little effort to introduce a rudimentary support for MLM, although at a significant performance loss \cite{voskuilen2018analyzing}. The component is also responsible for setting up and handling caches, coherence policies buses. Inside memHierarchy lies a memory controller which in turn is more appropriately named MemBackendConvertor. Here the conversion of application/CPU memory requests into memory backend specific requests are made. The backend is either a built-in memory model or an external tool, which supports, e.g., DRAMSim2, CramSim and the aforementioned HMCSim. It is also possible to use multiple backends simultaneously to enable further exploration of hybrid memory types. A memory backend is written specifically for any external memory simulator, and HMCSim uses its own as well.
\bigskip

With integration of HMCSim already present in the simulator, the need for drastic changes were quite small. It has previously been possible to configure multiple memory devices, but it has not had any effect. As such, apart from the need to add support in HMCSim, there also needed to be some adaption to SST. Furthermore, there was no way to address more than one cube from SST and allocations would only have been done to the first device in the topology. After some changes it is now possible to address one memory at a time using a unique identifier. In addition, an attempt was made to spread out data across multiple nodes by allocating data on devices at random virtual addresses. The idea was that since HMC used a closed page policy, it would not matter where data was allocated. However, applications optimised for spatial locality would see enormous performance losses since one piece of its data could be very close while another very far away. Thus it was determined that this way of allocating data would neither be viable from a simulation perspective, i.e. taking very long to run, nor realistic for a real system. In the end, only one memory unit is being used per simulation.
\bigskip

While the idea behind HMC networks is that a CPU node can be inserted just about anywhere in the network still holds true, the handling of links between devices had to be changed. SST could now only use half of the available links to send and receive data to the network, as opposed to using all links when interfacing a single cube. Data is being sent in parallel in all of the links as much as possible, but if one link gets full requests will be passed to another link. This is to try avoid bottlenecking memory requests. It would remain theoretically possible to add a processor anywhere in this network, but there has been no check whether SST's coherence mechanisms would still work when this network is used rather than SST's own. 
\bigskip

In addition, SST lacked functionality to start and stop simulating the program execution after a set number of instructions. Due to no support for SimPoints, one billion instructions was set as the limit. In order to minimise measuring only the start-up of the application, the simulator was also configured to skip ahead for one billion instructions as well as use a na√Øve cache warm-up of 100 million instructions. To be clear, max instructions was supported from the start, but to start PIN instrumentation after set number of instructions was not, and that was added. This was done by adding a check for how many instructions had gone by and intercepting that particular instruction and activate tracing.
\bigskip

Lastly, since HMC is meant to be used with a silicon interposer, the simulation setup used might not be realistic; it might not be feasible to add 8 HMCs to a single interposer, and if it were, they would not be used in a chain. Instead a more efficient topology would be implemented. In addition, memory sizes, latencies and configuration might not correspond to any real device since most of that information is proprietary. One advantage of using SST when exploring topologies is that arbitrary networks of compute and memory nodes can be used and as such it would be possible to setup such a network in the simulator.

%TODO: Mention alternative, perhaps more widely used, where program traces are created and then replayed in the simulator. Did not get this to work.

%TODO:
%Talk about the problems present in configuring and emulating behaviour from something that is made to be very secret. E.g. timings of DRAM, Idd, power, layout etc are all proprietary for all manufacturers. Lots of guessing involved. 


\subsection{HMCSim} \label{HMC-Sim}
HMCSim is, much unlike many other memory simulators available, primarily built with multidimensional memories in mind. This three dimensional approach enables a unique parallel access to banks, which is not accounted for in current memory simulation environments \cite{6969550}. 
TODO: Describe what it is and what it does, how it functions, and then mention what it has been lacking. 
TODO: Talk about what kind of "interfaces" has been added and what it adds. Talk about the how the interface (host->dev) from SST has been changed and how allocation is done (random?).
\bigskip

HMCSim was already integrated as a memory backend in SST and the work to allow communication between a CPU model and the memory simulator was already done \cite{voskuilen2018sst}. The HMCSim integration utilises custom opcodes from SST to send commands to the memory, which also corresponds to how a HMC would work in a real application. While HMCSim has supported configurations of multiple devices, they could not communicate with one another. There was an awareness of other devices, but requests that were destined for another device were turned into zombie requests. The first addition to HMCSim was to support forwarding and routing of packets. The implementation is pretty simple, and seeks primarily to support this specific use case. A communication path was setup between devices that were linked together, and half of the available links were used to forward memory operations to the next device in line. This leaves the last cube in the chain not fully utilising its links, and could be a place to add another host in the network without any additional loss. Furthermore, this effectively means that we have cut the available bandwidth in half as compared to only connecting to one device. Since HMC specification is not very strict on how communication is done within a network this is still an acceptable solution.
\bigskip

TODO: Create a beautiful picture of how different nodes connect to each other, with the zig-zag pattern that was made. This was due to communication needed to be made on the same link as it originated - technical limitation.
\bigskip

Now that there was support for inter-memory communication, there need be a way to set things up as well. The very basic case which is tested in here only allows for devices to be connected in chains. This is to show the worst case scenarios, and how multiple hops in a network will affect performance. A support for topologies was created, although not further explored. Possible apparent extensions to this implementation would be to allow for more topologies, or -- more importantly -- a way to easily create custom topologies. As mentioned earlier, the new implementation is not very flexible in its way of creating new topologies, as that involves smartly iterating over devices and links in order to get a functioning network. It is however possible, but it was decided that it would be enough to just measure worst-case scenarios.
\bigskip

TODO: Mention what numbers are being used and see if we can get back the source for those numbers! Also include HMC config in some kind of table, e.g. link speed, size, banks etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Benchmarks  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarks}
SPEC CPU2006. Will mainly use memory bound - which? Select one that is more compute bound as well. Keep down the number of benches!
\bigskip

Benches that are included:
MCF, lbm, libquantum, omnetpp, and perlbench (for showing that mainly memory bound benches are affected). Used test data set in order to keep execution time to a minimum.

A benchmark which measures a system's sustainable memory bandwidth is STREAM \cite{mccalpin1995memory}, which is a memory bound application. It is designed to perform memory operations which exceed the cache size, and thus generates a lot of last level cache (LLC) misses.

Depict a roofline model, if one exists already. Reference to what it is and source, ofc. Moar content!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Measurments  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Measurement approach}
Measure latency is done only in HMCSim, as we do not care about any latency that SST introduces as that is only for processing and cache. That makes gathering stats a little easier. There are separate APIs that enables latency stats. Will use average latency to compare between system configs. What are the configs going to be? Should we actaully compare DRAM and HMC, or just the allocation on multiple devices, i.e., the direct implication of introduced latency. TODO: Add cache warm-up as well.

TODO: Mention SimPonts here, as well as PinPoints. The first might not have been possible to use here directly, but the latter should be possible to use since it uses the already featured PIN tool in Ariel. This would require some additional work, however. 
