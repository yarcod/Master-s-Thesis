\chapter{Methodology}
Evaluating new memory technologies without having physical access to them requires simulation of their designs. There is always a measure of uncertainty and error when performing simulations, but it still gives a reasonable estimation of what to expect from the real hardware. Some argue that, e.g., cycle accurate simulators might not be the best approach in system evaluation \cite{weaver2008cycle}, but in this study all memories will be evaluated with the same errors, and thus the relationship between their performances should remain relevant. 
\bigskip

HMC is meant to be used as near memory -- being integrated close to the CPU -- and putting them in a chain might be a stretch. Although it would be both supported and justifiable from a technical point of view, it might be hard and/or expensive to implement this in reality. Having multiple jumps in a network, which inherently will increase latency, should after a certain threshold arguably be regarded as far-memory. This would then imply that we would need to either regard this system as a MLM or a NUMA and then there would need to be policies used for that as well. Alternatively there could be some intelligent prefetching added in the network to hide latency, but that would also increase network load. In this case we will simplify and regard all HMC nodes as near memory. In addition, we will only allocate memory on one cube at a time.
\bigskip

First in this chapter the simulation software for micro architectures and memory technology is described. Next, a number of benchmark applications that will be run in the simulators are selected. Finally, the process of gathering and interpreting results is defined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Simulators  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulators}
For simulation of the memory network, two simulators will be used: Structural Simulation Toolkit (SST) and HMCSim. The former is used for accurate microarchitecture simulation, while the latter is used to simulate the HMC memory. There were not many simulators for HMC available when this project started, which lead to the use of HMCSim. This simulator in turn was already integrated with SST and this significantly lowered the amount of development needed to start. There has since come up other alternative HMC simulators \cite{7544479} \cite{Yang:2018:HCH:3240302.3240319} and there has also been created similar tools to accommodate multiple memory cubes in a network setup \cite{Siegl:2017:BAF:3132402.3132403}. Even so, the flexibility and extensibility of both HMCSim and SST has enabled a good basis for this kind of evaluation.

\subsection{SST}
SST is an architectural simulator which supports execution of x86 binaries, through Intel Pin Tool \cite{Luk:2005:PBC:1065010.1065034}, towards different memory back-ends \cite{rodrigues2011structural}. It uses a modular front-end back-end framework, where each part can be replaced independently to model different architectural configurations. This modularity and the simulator's parallel simulation environment is enabled by the use of Message Passing Interface (MPI). This makes all communications event-based, and events are tagged with a delivery time. These events are only sent when needed and no real processing time is unnecessarily spent on future events \cite{11.1093/comjnl/bxr069}. Communication within the simulator uses a network model for routing information between components, which enables arbitrary architectural topologies. In addition, the router model can take network usage and congestion into account and monitor where and how often it occurs as well as if it caused any additional latencies.

TODO: The name of the computer model which supports execution of x86 binaries is \emph{Ariel}. 

TODO: We have changed memHierarchy inside SST to recognize multi-cube setups and to address one at a time. This component of SST Elements [REF?] is responsible for handling most everything associated with memory communication, e.g., caches, memory controllers and buses. In addition, this library supports setting up different coherence policies and is, like the rest of the simulator cycle-accurate. Adding a more complex memory structure, e.g. a multi-level memory, will decrease the simulators performance noticeably. 

TODO: Flexible in the memory backend, which already supports a number of different models and other tools. MemHierarchy is connected to the backend, and it is between these two components that modifications had to be made. Example of flexibility is that the memHierarchy has been adapted to support MLM as in \cite{voskuilen2018analyzing}.

TODO: there has not been real support for multiple devices previously; while it has been possible to configure more than one device before, they were never used and never interacted with one another. When adding more devices, both the connection between devices within HMCSim and the knowledge about the other devices towards SST had to be implemented. While the idea behind HMC networks is that a CPU node can be inserted just about anywhere in the network still holds true, the handling of links between devices had to be changed. More on this in the HMCSim section. SST could now only use half of the available links to send and receive data to the network, as opposed to using all links when interfacing a single cube. Data is being sent in parallel in all of the links as much as possible, but if one link gets full requests will be passed to another link. This is to try avoid bottlenecking memory requests. 

TODO: In addition, an attempt was made to spread out data across multiple nodes by sending requests to devices at random (How was this random issued? Physical or virtual addresses?). In the end, this was in order to allow for different memory topologies, more on that later. The idea was that since HMC used a closed page policy, it would not matter where data was allocated. However, applications optimised for spatial locality would see enormous performance losses since one piece of the data could be very close while another very far away. Thus it was determined that this way of allocating data would neither be viable from a simulation perspective, i.e. taking very long to run, nor realistic for a real system.

In addition to the changes to make SST work with HMCSim, SST lacked functionality to start and stop simulating the program execution after a set number of instructions. Due to no support for SimPoints, one billion instructions was set as the limit. In order to avoid measuring the wrong things in application, the simulator was also configured to skip ahead for 1 billion instructions. To be clear, max instructions was supported from the start, but the start trace after set number of instructions was not, and that was added. This was done by adding to the PIN interface a check for how many instructions had gone by and intercepting that particular instruction and activate tracing. 

TODO: Mention that since HMC is meant to be used with a silicon interposer, the latencies used might not be true; it might not be feasible to add 8 HMCs to a single interposer, and if it were, they certainly would not be used in a chain. Topologies!

TODO:SST uses PIN to interpret x86 binaries. It is not cycle accurate, ref, etc. Limited to PIN 2.x as 3.x had some weird requirements as compared to 2, which required a major rewrite of memory backend. Tried to include it myself. One advantage is that bench is executed as if in a real machine, apart from OS sched.
TODO: Mention alternative, perhaps more widely used, where program traces are created and then replayed in the simulator. Did not get this to work.

%TODO:
%Talk about the problems present in configuring and emulating behaviour from something that is made to be very secret. E.g. timings of DRAM, Idd, power, layout etc are all proprietary for all manufacturers. Lots of guessing involved. 


\subsection{HMCSim} \label{HMC-Sim}
TODO: Describe what it is and what it does, how it functions, and then mention what it has been lacking. Talk about what kind of "interfaces" has been added and what it adds. Talk about the how the interface (host->dev) from SST has been changed and how allocation is done (random?). 

HMCSim was already integrated as a memory backend in SST and the work to allow communication between a CPU model and the memory simulator was already done \cite{voskuilen2018sst}. The HMCSim integration utilises custom opcodes from SST to send commands to the memory, which also corresponds to how a HMC would work in a real application. While HMCSim has supported configurations of multiple devices, they could not communicate with one another. There was an awareness of other devices, but requests that were destined for another device were turned into zombie requests. The first addition to HMCSim was to support forwarding and routing of packets. The implementation is pretty simple, and seeks primarily to support this specific use case. A communication path was setup between devices that were linked together, and half of the available links were used to forward memory operations to the next device in line. This leaves the last cube in the chain not fully utilising its links, and could be a place to add another host in the network without any additional loss. Furthermore, this effectively means that we have cut the available bandwidth in half as compared to only connecting to one device. Since HMC specification is not very strict on how communication is done within a network this is still an acceptable solution. 

TODO: Create a beautiful picture of how different nodes connect to each other, with the zig-zag pattern that was made. This was due to communication needed to be made on the same link as it originated - technical limitation. 

Now that there was support for inter-memory communication, there need be a way to set things up as well. The very basic case which is tested in here only allows for devices to be connected in chains. This is to show the worst case scenarios, and how multiple hops in a network will affect performance. A support for topologies was created, although not further explored. Possible apparent extensions to this implementation would be to allow for more topologies, or -- more importantly -- a way to easily create custom topologies. As mentioned earlier, the new implementation is not very flexible in its way of creating new topologies, as that involves smartly iterating over devices and links in order to get a functioning network. It is however possible, but it was decided that it would be enough to just measure worst-case scenarios. 

TODO: Mention what numbers are being used and see if we can get back the source for those numbers! Also include HMC config in some kind of table, e.g. link speed, size, banks etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Benchmarks  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Benchmarks}
SPEC CPU2006. Will mainly use memory bound - which? Select one that is more compute bound as well. Keep down the number of benches!
\bigskip

Benches that are included:
MCF, lbm, libquantum, omnetpp, and perlbench (for showing that mainly memory bound benches are affected). Used test data set in order to keep execution time to a minimum.

A benchmark which measures a system's sustainable memory bandwidth is STREAM \cite{mccalpin1995memory}, which is a memory bound application. It is designed to perform memory operations which exceed the cache size, and thus generates a lot of last level cache (LLC) misses.

Depict a roofline model, if one exists already. Reference to what it is and source, ofc. Moar content!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Measurments  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Measurement approach}
Measure latency is done only in HMCSim, as we do not care about any latency that SST introduces as that is only for processing and cache. That makes gathering stats a little easier. There are separate APIs that enables latency stats. Will use average latency to compare between system configs. What are the configs going to be? Should we actaully compare DRAM and HMC, or just the allocation on multiple devices, i.e., the direct implication of introduced latency. TODO: Add cache warm-up as well.

TODO: Mention SimPonts here, as well as PinPoints. The first might not have been possible to use here directly, but the latter should be possible to use since it uses the already featured PIN tool in Ariel. This would require some additional work, however. 
