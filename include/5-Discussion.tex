\chapter{Discussion}
What IS the effect of introducing latency? How much of a performance hit do we face? Speculate in if performance scales with number of devices. Maybe talk about if it is a good idea to have a network of HMCs compared to just having a few HMCs close by and use IO interfaces for other types of storage instead. There are other types of topologies as well -- would they perform better? Decrease the max jumps and what-not.

Normal real world cases would have a NUMA-aware OS, which would decrease this impact a lot. But if an application would use a lot of memory, then this could actually be applicable. 

It should be mentioned that after the start of this thesis, Micron has ended their initiative in HMC -- the website for the technology does not longer exist for example. Micron are instead now working with HBM2E and newer technology. 

Related to above: There have been multiple uses for HMC specifically for its ability to host a completely custom logic die at the bottom of the stack. Near-memory computation is when the CPU still performs the logic but has close and fast access to the memory and thus can take advantage thereof. A variant of this is Processing In Memory (PIM), where computations are done inside or just by the memory, as would be the case if the HMC logic die were to calculate arithmetic results as well as control memory flow. There has been multiple studies of this use case and they mostly agree that performance and energy efficiency increases when using HMC for specific workloads \cite{7917248} \cite{Min:2019:NEH:3287624.3287642} \cite{7804052} \cite{oliveira2017nim}. This further justifies the ability to have several cubes in a network as the latency of memory access could be less sensitive if some or all of the computations are done locally in the stacks.

TODO: Could be interesting to add that it has been investigated in the same simulator whether MLMs actually add any performance to a system. It is quite extendable as such and could be used for future work. Using HBM from DRAMSim2 instead of HMCSim could perhaps give other results, as compared to using HMC which they were using (I think) \cite{Awad:2017:PAU:3132402.3132422}. Perhaps also speculate whether a HMC memory controller could act as the MLM manager, deciding whether a specific allocation should end up in fast or slow memory. Applicable to HBM as well? Although it being a slower type of memory.

--------------------------------------

The fact that a single, memory bound application's performance is hampered by increased bandwidth through parallelism is interesting. It would have been interesting to see how performance was affected when running multiple applications simultaneously, as well as adding some real scheduling on memory accesses.