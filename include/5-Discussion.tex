\chapter{Discussion}
Using memories in a network has produced some interesting results, not at all in line with expectations. First of all, applications have a built-in latency sensitivity due to their design, but their memory access patterns can also affect the speed by which the memory can serve results. Using HMC's packet-based protocol, there need to be queues to receive and handle incoming requests. These queues in turn will largely determine how fast the memory can act on incoming packets, because as soon as queues start to fill up there will be an increased waiting time for every new request. The waiting time for every new request is the sum of the serving time for all previous requests in that queue, and adding to that is the request's own serving time. Moreover, there can arise additional latencies for a number of reasons, e.g. if internal routing inside the cube has to be done. Furthermore, there is an uncertainty of what amounts of latency an application will see. A largely CPU bound application like namd manages to get very high average latency, as opposed to mcf which gets an, arguably, low average latency. It comes down to the number of requests sent in a short time period, and thus how fast the queues fill up. This is all running non-optimised software, which will use the smallest request size of 16B. Using even larger request sizes takes more buffer space and time to convert into packets, which further increases uncertainty but also increases effective bandwidth \cite{8167757, 8366939}. In the end, the latency results are underwhelming when comparing to the sub-100 ns that DRAM can achieve under normal circumstances.
\bigskip

The results obtained indicate that data allocated far away from the CPU does not affect average latency more than the added transmission time, almost regardless of how many links are being used between devices. While this might be the case, and the gained bandwidth generally does not outweigh the added latency, the access time patterns for data accesses on the first and third device look too alike for it to entirely be a coincidence. There has been a limitation in SST where bandwidth has been hard to utilise if parameters are not setup with great care, and it might be that we simply do not get enough extra bandwidth to use \cite{hammond_rodrigues_hemmert}. Furthermore, using small packet sizes means that theoretical maximum bandwidth is a lot harder to achieve, and as such the effects of network traversal might be too large to overcome to maintain a good bandwidth. In addition, because we with our setup limited the number of links to a maximum of four -- as opposed to the standard's maximum of eight -- we thereby knowingly decreased available bandwidth. Moreover, the additional internal routing observed when using one device and four links was masked by performing network hops, which could be attributed to how the network handling is slightly different between host and HMC compared to that between cubes. Unable to find evidence of faulty network handling in either software, this topic would require further investigation in order to fully affirm our test results. Summing up, because of limitations in the simulators, it might be that there was not enough bandwidth available to see the expected levels of performance improvements. 
\bigskip

Having a setup with many memory devices on the same substrate is currently unfeasible, and it would probably be ill-advised to use a chain topology. As such, this is a most theoretical approach to investigate worst-case scenarios. In a real system there would be fewer devices -- possibly paired with a more effective topology if there are more than one memory device -- which are used either by themselves or in tandem with another memory type. Moreover, having a packet based interface adds overhead and in some cases decrease performance, but could enable scalable, possibly MLM, solutions to be more easily created \cite{8167757}. However, in order to actually benefit from a switched network it is crucial that latency is kept at a minimum. Accepting the overall higher latencies, it would be interesting to further explore improved topologies in combination with scheduling policies which would maintain the number of nodes but possibly decrease overall latency. In addition, since HMC networks support adding compute hosts anywhere, it would be interesting to both measure performance when having cubes at different distances, i.e. devices clustered around a CPU and having a larger bus in between, and when there is shared data between these nodes.
\bigskip

In this thesis we have used a very naive and simple approach to memory allocation as well as link utilisation, but ordinarily an operating system, compiler, software or hardware would handle memory accesses more effectively. In addition, we run single applications in each test, which, with our approach, likely do not fully use the parallelism and inherently bandwidth advantage that HMC presents. Furthermore, applications which do not stress the memory as much, e.g. namd, and are less sensitive to slow memory could be allocated further away than other, more sensitive programs. A NUMA aware OS could prioritise where applications should be allocated and where the raw bandwidth and low latency is needed the most. The results we attained showed that single applications' performance could be hampered without proper scheduling. Another way to improve link utilisation effectiveness would be to prefetch data from far-away devices such that it is available a lot closer than otherwise. This way, the decreased latency available close to the CPU could be used more efficiently. Additionally, links which are underutilised could help with prefetching in order not to encumber links fetching data. 
\bigskip

There have been multiple use cases for HMC for its ability to host a completely custom logic die at the bottom of the stack. Our setup with HMC devices very close to the host is called near-memory computation, which means that the CPU still performs the arithmetic but has close and fast access to the memory and can take advantage thereof. Closely related is Processing In Memory (PIM), where computations are done inside or by the memory itself, as would be the case if the HMC logic die were to perform logic operations as well as memory control flow. There have been multiple studies of this use case and they mostly agree that performance and energy efficiency increases when using HMC for specific workloads \cite{7917248, Min:2019:NEH:3287624.3287642, 7804052, oliveira2017nim}. This further justifies the ability to have several cubes in a network as the latency of memory access could be less sensitive if some or all of the computations are done locally in the stacks. From the results obtained in this work, we determine that the most crucial parts of the network is queue congestion, being able to minimise the number of memory requests by doing part of the work \emph{in memory}. Furthermore, we can see that while there indeed is an effect to application performance when increasing distance to memory, there appear to be no other discrepancies and it should be possible to mask this latency with PIM functionality. This could be done with HMC's built-in CMC functionality, which could be setup to supply arithmetic commands to the logic layers.
\bigskip

Lastly, it should be mentioned that after the start of this thesis, Micron has ended their initiative in HMC. Micron are instead now working with HBM2E and newer technology, and HBM has been generally accepted by the industry as the path forward. While HBM does not support networks of memory devices, it still has a logic die at the bottom of the stack and could also implement PIM functionality if need arises.