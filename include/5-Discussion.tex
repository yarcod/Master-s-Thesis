\chapter{Discussion}
Using memories in a network have proven to affect single applications lesser than expected. While there are detectable performance degradations by increasing minimum memory latency, the increase in average numbers mostly stems from distance and less from contention and therefore there are at most tendencies to measure. One example of such tendency is that mcf, as expected, is more latency sensitive and has a higher degree of impact than other applications. In addition, the memory intensity of the application seems to reflect in visible amount of contention. In contrast to this, namd, which is mostly a cpu bound application, sees quite a small impact with the added latency as well as low amounts of contention. The tendency for namd, from figure \ref{All-apps-latency-trends}, is that the degradation in performance is lower than that of the other, more memory bound applications. While this thesis has not been aiming to validate applications' latency sensitivity, these results somewhat confirms what sources have said of the difference CPU2006 benchmarks.
\bigskip

What then is the effect introducing a network of high speed memories to non-optimised software running on a barebone (simulated) hardware? While program performance itself has been too difficult to determine and compare, memory latency at least has some direct impact on execution time. The most prominent impact is that although it is quite a parallel system, contention is still created and not necessarily alleviated by adding more links. On the contrary, there seems to be an adverse effect where too many links increases waiting time when accessing data close to the CPU. Furthermore, this behaviour is masked when having multiple hops present, and as such adding more links while having to move in the network does appear to have minimal to no effect on the outcome. The results obtained indicate that single applications without optimisations or an OS level scheduler does not benefit from adding MLP. Additionally, average latency numbers obtained for different configurations suggests that performance scales well with added network nodes and that it is more the software's sensitivity that determines the performance penalty. 
\bigskip

Having a setup with many memory devices on the same substrate is currently unfeasible, likewise it would probably be ill-advised to have all of them in a chain topology, and as such this is a most theoretical approach to investigate worst-case scenarios. In a real system there would be fewer devices -- possibly paired with a more effective topology if there are more than one memory device -- which are used either by themselves or in tandem with another memory type as well. Using network-capable memories by itself could be a good idea as to increase the level of abstraction for the entire memory system and thus enable a more flexible memory hardware solution, but the problem is still that this requires changes in a lot of today's established ecosystems. Having a packet based interface would also add some overhead, and in some cases decrease performance, but it would also let scalable, possibly MLM, solutions to be more easily created \cite{8167757}. However, in order to actually benefit from this switched network it is crucial that latency is kept at a minimum. Therefore it would be particularly interesting to further investigate improved topologies which would maintain the number of nodes but decrease overall latency. In addition, since HMC networks support adding compute hosts anywhere, it would be interesting to both measure performance when having cubes at different distances, i.e. devices clustered around a CPU and having a larger bus in between, and when there is shared data between these nodes.
\bigskip

In this thesis we have used a very naive and simple approach to memory allocation as well as link utilisation, but ordinarily an operating system, compiler, software or hardware would handle memory accesses more effectively. This way much of the created contention in our results would to a large extent be limited and thereby improve the results. However, one other missing piece from a live system is that we only run single applications in each test. This means that we, with our approach, likely do not use the parallelism and inherently bandwidth advantage that HMC presents. Additionally, since applications which do not use memory as much, e.g. namd, are less sensitive to slow memory, these could be allocated further away than other, more sensitive programs. A NUMA aware OS could prioritise which apps should go where, and where the raw bandwidth and/or low latency is needed the most. The results we attained showed that single applications' performance could be hampered without proper scheduling. Another way to make utilisation more effective would be to prefetch data from far-away devices such that it is available a lot closer than otherwise. This way, links which are underutilised could help with prefetching. 
\bigskip

There have been multiple use cases for HMC for its ability to host a completely custom logic die at the bottom of the stack. Near-memory computation is when the CPU still performs the logic but has close and fast access to the memory and thus can take advantage thereof. A variant of this is Processing In Memory (PIM), where computations are done inside or just by the memory, as would be the case if the HMC logic die were to calculate arithmetic results as well as control memory flow. There has been multiple studies of this use case and they mostly agree that performance and energy efficiency increases when using HMC for specific workloads \cite{7917248, Min:2019:NEH:3287624.3287642, 7804052, oliveira2017nim}. This further justifies the ability to have several cubes in a network as the latency of memory access could be less sensitive if some or all of the computations are done locally in the stacks. From the results obtained in this work, we can see that there indeed are an effect to applciation performance when there is a distance, but since there appear no other disrepeencies it should be possible to mask distance with PIM function. 
\bigskip

It should be mentioned that after the start of this thesis, Micron has ended their initiative in HMC -- the website for the technology does not longer exist for example. Micron are instead now working with HBM2E and newer technology, and HBM has been generally accepted by the industry as the path forward. While HBM does not support networks of memory devices, it still has a logic die at the bottom of the stack and could still implement PIM logic.

TODO: Again, related speculation, many small compute nodes with their own memory stack, like HMC and a few purely computation centric devices would be an interesting architecture, combining PIM with normal applications.
