\chapter{Discussion}
Using memories in a network have proven to affect single applications to a lesser extent than expected. While there are detectable performance degradation by increasing minimum memory latency, the increase in average numbers mostly stems from distance and less from contention. One clear example is that mcf, as expected, is more latency sensitive and has a higher degree of impact to its execution time than other applications. In contrast to this, namd, which is mostly a CPU bound application, sees quite a small impact with the added latency. The tendency for namd, from figure \ref{All-apps-latency-trends}, is that the degradation in performance is lower than that of the other, more memory bound applications. While this thesis has not been aiming to validate applications' latency sensitivity, these results to some extent confirms what has been said about the CPU2006 benchmarks used.
\bigskip

What then is the effect introducing a network of high speed memories to non-optimised software running on a barebone (simulated) hardware? There is a clear impact to application performance when increasing latency, as expected. However, one other aspect affecting performance is applications' different need for BLP and how much data they need to access at a time. Namd and LBM likely would have run faster if they were able to access larger portions of data in one request, resulting in fewer DRAM accesses. With HMC's close page policy, whenever a memory access stream wants data larger than what can be accessed in parallel, every row is closed and needs to be opened again for every subsequent access. At the same time there seems to be an adverse effect where too many links increases waiting time when accessing data close to the CPU. Furthermore, this behaviour is masked when having multiple hops present, and as such adding more links while having to move in the network does appear to have minimal to no effect on the outcome. The results obtained shows that single applications without optimisations or an OS level awareness does not benefit when data placement and access paths and not considered. Additionally, average latency numbers obtained for different configurations suggests that performance scales well with added network nodes and that it is more the software's sensitivity that determines the performance penalty. 
\bigskip

Having a setup with many memory devices on the same substrate is currently unfeasible, likewise it would probably be ill-advised to have all of them in a chain topology, and as such this is a most theoretical approach to investigate worst-case scenarios. In a real system there would be fewer devices -- possibly paired with a more effective topology if there are more than one memory device -- which are used either by themselves or in tandem with another memory type as well. Using network-capable memories by itself could be a good idea as to increase the level of abstraction for the entire memory system and thereby enable a more flexible memory hardware solution, but the problem is still that this requires changes in a lot of today's established ecosystems. Having a packet based interface would also add some overhead, and in some cases decrease performance, but it would also let scalable, possibly MLM, solutions to be more easily created \cite{8167757}. However, in order to actually benefit from this switched network it is crucial that latency is kept at a minimum. Therefore it would be particularly interesting to further investigate improved topologies which would maintain the number of nodes but decrease overall latency. In addition, since HMC networks support adding compute hosts anywhere, it would be interesting to both measure performance when having cubes at different distances, i.e. devices clustered around a CPU and having a larger bus in between, and when there is shared data between these nodes.
\bigskip

In this thesis we have used a very naive and simple approach to memory allocation as well as link utilisation, but ordinarily an operating system, compiler, software or hardware would handle memory accesses more effectively. This way much of the created contention in our results would to a large extent be limited and thereby improve the results. However, one other missing piece from a live system is that we only run single applications in each test. This means that we, with our approach, likely do not use the parallelism and inherently bandwidth advantage that HMC presents. Additionally, since applications which do not stress the memory as much, e.g. namd, are less sensitive to slow memory, these could be allocated further away than other, more sensitive programs. A NUMA aware OS could prioritise which apps should go where, and where the raw bandwidth and/or low latency is needed the most. The results we attained showed that single applications' performance could be hampered without proper scheduling. Another way to make utilisation more effective would be to prefetch data from far-away devices such that it is available a lot closer than otherwise. This way, links which are underutilised could help with prefetching. 
\bigskip

There have been multiple use cases for HMC for its ability to host a completely custom logic die at the bottom of the stack. Our setup with HMC devices very close to the host is called near-memory computation, which means that the CPU still performs the arithmetic but has close and fast access to the memory and can take advantage thereof. Closely related is Processing In Memory (PIM), where computations are done inside or by the memory itself, as would be the case if the HMC logic die were to perform logic operations as well as memory control flow. There have been multiple studies of this use case and they mostly agree that performance and energy efficiency increases when using HMC for specific workloads \cite{7917248, Min:2019:NEH:3287624.3287642, 7804052, oliveira2017nim}. This further justifies the ability to have several cubes in a network as the latency of memory access could be less sensitive if some or all of the computations are done locally in the stacks. From the results obtained in this work, we can see that there indeed is an effect to application performance when increasing distance to memory, but since there appear to be no other discrepancies it should be possible to mask this latency with PIM functionality. However, it should not be as sensitive to poor scheduling as our near-memory setup since the memory network would simply be used for data forwarding, assuming that the logic layers still need to provide calculated results to the host. TODO: Talk about how CMC could be used to get *some* PIM effect to start with. 
\bigskip

It should be mentioned that after the start of this thesis, Micron has ended their initiative in HMC -- the website for the technology does not longer exist for example. Micron are instead now working with HBM2E and newer technology, and HBM has been generally accepted by the industry as the path forward. While HBM does not support networks of memory devices, it still has a logic die at the bottom of the stack and could still implement PIM logic.
