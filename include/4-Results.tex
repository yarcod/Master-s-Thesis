\chapter{Results}
Simulating real applications in virtual hardware without any real interaction is hard and there are many things that can break along the way. The simulations were run on a by Chalmers provided compute cluster and took quite some time to complete. This cluster luckily used an older version of Ubuntu with a relatively old kernel, and since PIN 2 depends on older kernel functionality it meant it could be run without a hitch. In addition, in order to support the old ABI used with some of its binaries while having full support for C++11, GCC 4.9.4 had to be used, specifically. 
\bigskip

TODO: In this chapter we will first measure what latency behaviour looks like when using one single HMC device, with a single link hop. Then we will increase the number of devices and always use the device at the end of the chain, e.g. when having 8 devices we will perform all allocations on the 8th device. We will view these results both from a device and a application perspective. Finally, we present a short summary of our findings.

\section{Simulations}
The simulations were run with multiple configurations, where every application (stated in \ref{method-benches}) was run with either 4 or 8 links configured, enabling between 1 and 6 memory cubes and data was allocated on the last memory unit in the network. The reason for testing both 4 and 8 links was to see whether the increased bandwidth, and in this case amount of parallelism, would significantly impact the latencies. Time of flight is measured from the memory controller's point of view, i.e., from when it was first sent from the host to when the response arrived back. 
\bigskip

\subsection{MCF}
The first application to be run is the, arguably, most memory intensive and latency sensitive benchmark in the SPEC2006 suite - 429.mcf. Figure \ref{Memory-access-429-single}, visualising access times when a single HMC device is being used, shows that there is a lot of latency going on during application run time, even though the only contention available is from its own memory requests. Additionally, there are notable peeks present at precisely 93 ns intervals. This could be explained as pointer chasing, where first hit is pointer lookup and the second hit, i.e. 93 ns later, is the data lookup. Between the peaks the latency patterns look quite similar. Having just one memory device is the most efficient solution available in this situation, hence we denote this as an optimal solution.
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/429-x.4-1.jpg}
    \caption{Access times using one device with four links, running 429.mcf.}
    \label{Memory-access-429-single}
\end{figure}

Adding more devices, and inherently placing data fruther from the host, will incurr greater latencies as seen in figure \ref{Memory-access-429}. However, it can be noted that the peaks all have about the same magnitude and the repeating pattern in between is reminescent of each other but with a few differences. These variations should be due to contention arising on the links when data to and from far-away devices tries to use the same links. It should also be noted that the contention looks different depending on distance, and while it mostly follows the access pattern from the optimal case it adds latencies at different places. Looking closer, in figure \ref{Memory-access-429-double}, where the access patterns of one and two devices respectively are put together, we can better distinguish between the patterns. We make sure to align the graphs in order to compensate for the hops distances. With the trend line for each setup we can see that there overall is some added latency with contention on the routing links. This gives rise to additional loss of performance. Interestingly, for this benchmark application, contention does not increase linearly with added number of devices, i.e. hops. Using four devices will increase the overall contention just slightly, while having six devices makes contention notably higher. The overall application performance still suffers from the added latency that multiple, long hops brings.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/429-x.4.jpg}
    \caption{Comparing access times when using one, two, four and six devices.}
    \label{Memory-access-429}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/429-x.4-2.jpg}
    \caption{Part of the access patterns for one and two devices, where the latter has been adjusted to compensate for initial latency.}
    \label{Memory-access-429-double}
\end{figure}

Next we run using more links between the host and first device, as well as between individual cubes. Specifically, we now simulate using eight links instead of the previous four, of which four respectively two were usable (as explained in \ref{HMC-Sim}). This should to some extent alleviate the contention issue, but since the access scheme of the links is a simple round robin there might also be some added waiting time before data gets to the host. Running mcf using eight links and more than four devices turned out to take too long time (excess of 2 months run time) and those simulations were finally terminated.
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0\linewidth]{figure/429-2.4-8.jpg}
    \caption{Part of the access patterns for one and two devices, where the latter has been adjusted to compensate for initial latency.}
    \label{Memory-access-429-link-compare}
\end{figure}

Running mcf using eight links produces some odd results. As can be seen in figure \ref{Memory-access-429-link-compare} the access pattern looks very much like using only four links, but there are new tailing spikes at 9 ns later. The second spike is a split of the otherwise always present 93 ns interval peak, where close to 20\% of all accesses are delayed with 9 ns. This could be due to how the host accesses each link with a round robin policy and that there is an additional wait introduced when the concurrency of multiple links cannot be utilised. By increasing bandwidth, through increased parallelism, we potentially worsen performance for this single application. However, when using two cubes, i.e. adding a hop, the patterns look almost identical. As such, the added latency of multiple links is cancelled out when having longer network traversal time.
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/429-averages.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-429-average-latency}
\end{figure}

Average latency increases with added network hops, as can be expected, but it also increases somewhat when utilising more parallelism, as seen in figure \ref{Memory-access-429-average-latency}. Although using two or more devices masked the specific behaviour from figure \ref{Memory-access-429-link-compare}, the average latency still takes a hit when the application is unable to use the available bandwidth.

\subsection{NAMD}
Namd is a mostly compute bound application. This should make access patterns look a little different based on how much data is accessed at a time. In figure \ref{Memory-access-444} we can see what those patterns look like using one, three, five and seven cubes in the network. While the patterns look quite similar, and while using one single device appears to provide the best results, the number of memory requests are simply too low for it to make any substantial difference. Moreover, there appears to be relatively few requests simply returning accessed memory requests immediately, i.e. having a latency of 93 ns, which worsens the application's overall performance severly. Most requests are served after 186 ns, which is double that of the minimal possible latency. Namd performs a lot of array lookups and it could be this often-accessed array we are seeing being used. Additionally, since Namd is a serial benchmark, it might be that the introduced MLP only hurts performance. Sadly HMCSim cannot be configured using one single link and thus there is no easy way to verify this claim here.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{figure/444-x.4-1.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-444}
\end{figure}

Adding even more links, using eight instead of four, will further hamper the application. Interestingly, the same behaviour as was seen in the mcf results are seen in figure \ref{Memory-access-444-links-compare} as well. There are new peaks added 9 ns after the otherwise normally present peaks, but these disappear as more devices are added to the network. This spike has a much greater impact to this application's average latency compared to mcf, adding 5 ns as compared to 2 ns. However, in the end, as can be seen in figure \ref{Memory-access-444-average-latency} the biggest hit to performance is taken when introducing new hops. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/444-2.4-8.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-444-links-compare}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/444.averages.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-444-average-latency}
\end{figure}

\subsection{SOPLEX}
Soplex is a linear program where every iteration depends on the last one. As such, while the application is memory intensive, increased MLP should not yield any significant improvements for the general algorithm. However, when starting to look at the average latencies we notice an unexpected result, as seen in figure \ref{Memory-access-450-average-latency}. Having four devices with eight links generates, marginally, better results with respect to the average latency. It is hard to determine what exactly made the difference, since, as can be seen in the comparative figure \ref{Memory-access-450-4-dev-4-8-links}, the access patterns between the two look more or less identical. This resembles what other applications have showcased, that increased hop distance masks latencies added by contention. The average values are rounded down as well and the real difference isn't that big. In the end, this result could be interpreted as being within the margin of error and the real results be seen as more or less equal.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/450-averages.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-450-average-latency}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/450-4.4-8.jpg}
    \caption{Nearly identical latencies with four and eight links when using four devices.}
    \label{Memory-access-450-4-dev-4-8-links}
\end{figure}

Otherwise we see a repeated pattern in figure \ref{Memory-access-450-2-dev-4-8-links}, where additional spikes arises 9 ns behind the largest peaks when using eight links and allocating data close to the processor. This also affects the average latency in this configuration, making it 3 ns slower compared to using only four links due to this phenomena. As seen before, however, this behaviour seem to be isolated to having more links close to the processor. Otherwise, it does not look like soplex neither suffers nor gains from having access to more links. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/450-2.4-8.jpg}
    \caption{Using eight links close to the processor creates additional spikes.}
    \label{Memory-access-450-2-dev-4-8-links}
\end{figure}

\subsection{LBM}
The results for lbm are quite similar to that of namd, except that the former has a lot more memory requests than the latter. This both means that the graphs resembles one another to some extent but the otherwise prominent peaks at 93 ns intervals start to become irrelevant a lot sooner with namd. This both implies that the application's algorithm is both memory intensive and likely relies a lot on arrays and/or pointers. This means that most data lookups are stored in a data structure requiring accessing pointers and thereby increases the number of DRAM accesses inside cubes. Having even longer hops does not affect performance any more significantly than pointer chasing already does, as can be seen when comparing the average latencies in figure \ref{Memory-access-470-averages}. Furthermore, when studying the access pattern of different network configurations in figure \ref{Memory-access-470} we see, once again, that the patterns are very similar and that the biggest peak is at twice that of the optimal latency.
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/470-averages.jpg}
    \caption{Comparing average latencies between configurations.}
    \label{Memory-access-470-averages}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/470-x.4.jpg}
    \caption{Access patterns between configurations.}
    \label{Memory-access-470}
\end{figure}

The same behaviour regarding using four and eight links close to the cpu, i.e. only having one device, is present when running lbm. There appears to be the same loss in performance both to contention as well as the additional peaks. Lastly, it should be noted that running lbm using eight links and six devices took too long to complete and the session was terminated without any result available. However, the trends present among the other results, the outcome does not look very difficult to guess.

\subsection{OMNETPP}
Finally, omnetpp is run and while the results look similar to other applications there is one anomality seen in figure \ref{Memory-access-471}. The trend lines show that there is a bigger gap between the configurations than we have seen previously, indicating that this application is causing more contention along the hops than the others have done. Since omnetpp, an application modeling large Ethernet networks, inherently supports parallelisation it might be that the application tries to do many things simultaneously. This could be conflicting with the single core host set up and could generate some waiting already at run time. 
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/471-x.4.jpg}
    \caption{Access patterns between configurations.}
    \label{Memory-access-471}
\end{figure}

Viewing figure \ref{Memory-access-471-link-compare}, which compares access times using four and eight links, it looks like adding MLP is unusually forgiving in terms of the size of the second spike. Comparing link configurations when having multiple hops gives more or less identical results, and the contention created by having more links available is hidden by the latency of network traversal. Despite this, the average latencies for the different setups follows a similar pattern as previous runs, as seen in figure \ref{Memory-access-471-averages}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/471-2.4-8.jpg}
    \caption{Access patterns between configurations.}
    \label{Memory-access-471-link-compare}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/471-averages.jpg}
    \caption{Access patterns between configurations.}
    \label{Memory-access-471-averages}
\end{figure}

% A first test was run with 429.mcf, which is arguably the most memory intense and latency sensitive benchmark out of the chose applications, just to see how the system's simulated memory would behave. This test was run using one active link and data allocated on the closest device. Figure \ref{Memory-access-behaviour} shows a graph over the number of times a specific access time, in ns, was reached for all simulated memory requests. The pattern seen is a more or a less repetitive form which reappears with 93 ns interval. The single link means that we only are connected directly to a single vault, and accesses to memory locations belonging to other vaults will have to be routed in the mesh network. However, since HMCSim probably does not perform a DRAM lookup -- which would incur a 93 ns latency, given our configuration -- for a request not belonging to the single vault, it is more likely that this pattern is due to HMC's Custom Memory Commands (CMC). A few such are implemented in HMCSim, e.g. for atomic access to multiple pages, and these will perform a new lookup for every requested page. The high amount of latency this adds is most likely due to HMC using a closed page policy, meaning that there will be no hits in the row buffer and even sequential data will have to fetched again.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=0.75\linewidth]{figure/Testdata-4dev-4or1link-alloc0.png}
% \caption{Number of occurrences for different memory access times. }
% \label{Memory-access-behaviour}
% \end{figure}

\section{Summary}
The behaviour seen between the different applications is remarkably similar. All runs provide an access latency pattern which all have the same main points. There will be an initial spike at the shortest possible latency with an arcuate, downward shape ending in another peak 93 ns later. This latter peak is either an order of magnitude bigger or smaller, depending on the application and what data and algorithm it is based on. Next there is again an arcuate shape very similar to the previous one ending in a peak 93 ns later. This repeats until there are too few memory accesses to make a comparison worthwhile. This happens regardless of how many links or cubes there are in the network, with one important exception: adding four extra links, i.e. using eight links, while allocating data close to processor will result in added waiting and a possible loss of performance. This could be due to the basic setup of the simulator, where the memory controller naively uses a round robin policy to check each link in turn for data. TODO: Could be that single core cpu makes MLP really hard to use?
\bigskip

A summarised view of the average latencies can be seen in figure \ref{All-apps-latency}, where they all seem to portay the same amount of increase for each added hop. However, studying the increase in average latency in more detail, in figure \ref{All-apps-latency-trends}, we can see that some applications have a slightly higher increase than others. Specifically, mcf, which is supposed to be latency sensitive, shows that it does suffer from the longer access times. While the results obtained does seem to support this claim, it should be noted that the mcf trend line in figure \ref{All-apps-latency-trends} is based on fewer results than other applications.
\bigskip

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/all-apps-latencies.jpg}
    \caption{Access patterns between configurations.}
    \label{All-apps-latency}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/all-apps-latencies-trends.jpg}
    \caption{Access patterns between configurations.}
    \label{All-apps-latency-trends}
\end{figure}

Having just one HMC device in the network will let CPU get access to a high speed main memory and provides performance increases in relation to how memory bound an application is, similar to that of having a normal DDRx memory available albeit at a lower bandwidth. One of the biggest differences lies in that fact that the memory controller when using HMC is placed beneath the memory stack. There is an advantage in bus utilisation where having HMC's abstract network layer decreases the number of trips to host. Traditionally, CPUs need to determine if something is a pointer, consult the TLB if it is present and otherwise access main memory for it. With this approach, we instead leave out the extra trip back to CPU and instead allow the memory controller (logic layer) to determine this. This helps to limit the contention created by pointer chasing.
\bigskip

The biggest increase in contention in a memory network, with this topology at least, comes from adding hops. Contention rises noticeably just by adding one single cube, as can be seen in figure \ref{Memory-access-429-double}, but every successive cube will have maginal impact on contention. Taking a look at, e.g., figure \ref{Memory-access-471}, we see that the gap between trend lines for one and two devices initially is bigger than that of the lines for two and three devices. It is also visible from the graph that the access latency patterns look very much the same and is not impacted all that much. Furthermore, contention does not seem to be alleviated by utilising more links when having to jump to an adjacent memory cube. As seen in figure \ref{Memory-access-450-4-dev-4-8-links}, the patterns look more or less identical, within the margin of error. 